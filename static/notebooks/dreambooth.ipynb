{"cells":[{"cell_type":"markdown","id":"bd3d5db3","metadata":{"id":"bd3d5db3"},"source":["\n","# DreamBooth with Stable Diffusion\n","\n","[DreamBooth] is a personalization technique‚Äîintroduced by Google Research and Boston University in 2022‚Äîthat fine-tunes a full text-to-image diffusion model (such as Imagen or Stable Diffusion) using just a handful (typically 3-5) of reference images. By associating a unique identifier token with the subject's class (for example, ‚Äúa photo of \\[XYZ] dog‚Äù), the model learns to faithfully generate that specific subject in novel contexts, poses, and lighting conditions. This allows for high-fidelity, subject-driven image synthesis even with minimal training data.\n","\n","[DreamBooth]: https://en.wikipedia.org/wiki/DreamBooth?utm_source=chatgpt.com \"DreamBooth\"\n","\n","## What We're Going to Do\n","\n","1. Install dependencies\n","2. Upload your training images (3-10 high-quality images of your subject)\n","3. Train a DreamBooth model on your subject\n","4. Generate new images of your subject\n"]},{"cell_type":"code","execution_count":null,"id":"ec56c997","metadata":{"id":"ec56c997"},"outputs":[],"source":["# @title Install Dependencies {\"display-mode\":\"form\"}\n","\n","# @markdown **Run this first.** We need to import _all_ the essential Python libraries and modules required for DreamBooth training and image generation. It brings in tools for handling operating system tasks, math operations, randomness, and numerical computing (like `numpy`). It includes PyTorch for deep learning, Hugging Face‚Äôs `diffusers` and `transformers` libraries for working with Stable Diffusion and text/image processing, and utilities for dataset handling, image manipulation (Pillow), progress tracking (`tqdm`), and training optimizations such as gradient checkpointing and learning rate scheduling. Collectively, these imports set up the foundation needed to prepare datasets, train a DreamBooth model, and generate images from text prompts.\n","\n","%pip install -q git+https://github.com/huggingface/diffusers\n","%pip install -q accelerate tensorboard transformers ftfy bitsandbytes\n","%pip install xformers -q --index-url https://download.pytorch.org/whl/cu124\n","%pip install -qq bitsandbytes\n","\n","# ‚úÖ Verify\n","import torch, torchvision, torchaudio, xformers\n","from google.colab import (files, output)\n","\n","output.clear()\n","\n","from pathlib import Path\n","import argparse # Used for parsing command-line arguments (though used here via Namespace)\n","import itertools # Provides tools for creating iterators for efficient looping\n","import math # Provides mathematical functions\n","import os # Provides a way to interact with the operating system (e.g., file paths)\n","from contextlib import nullcontext # A context manager that does nothing, useful as a placeholder\n","import gc # Garbage collection so we don't run out of memory.\n","\n","import random # Used for generating random numbers, potentially for seeding\n","import numpy as np # A fundamental package for scientific computing with Python, used for numerical operations\n","import torch # The core PyTorch library for deep learning\n","import torch.nn.functional as F # Provides a collection of functions for neural networks\n","import torch.utils.checkpoint # Used for gradient checkpointing to save memory during training\n","from torch.utils.data import Dataset # An abstract class representing a dataset, used for creating custom datasets\n","\n","import PIL # Pillow library, used for image manipulation\n","from accelerate import Accelerator # Hugging Face library for simplifying distributed training\n","from accelerate.logging import get_logger # Function to get a logger for logging training progress\n","from accelerate.utils import set_seed # Function to set the random seed for reproducibility\n","from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel # Diffusers library components for diffusion models\n","from diffusers.optimization import get_scheduler # Function to get a learning rate scheduler\n","from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker # Component for checking generated images for safety\n","from PIL import Image # Image class from Pillow\n","from torchvision import transforms # Provides common image transformations\n","from tqdm.auto import tqdm # A library to display progress bars\n","from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer # Transformers library components for working with text models\n","\n","print(\"üë®‚Äçüé§ Environment ready to rock.\")"]},{"cell_type":"markdown","id":"7794abec","metadata":{"id":"7794abec"},"source":["## Upload Your Images\n","\n","Next up, we're going to upload some pictures of our subject. It can be a person, a dog, your favorite sock puppet. I don't care. Choose 3‚Äì10 images of your subject to upload. **Fun Fact**: Google Colab has a built-in way for you to upload files directly into your notebook's storage.\n","\n","This code snippet creates a dedicated folder to store training images, then checks whether the folder is empty before prompting you to upload new files. If no images are present, it opens a file picker for you to select and upload pictures from your computer, otherwise it skips the upload process. Throughout, it prints status messages to confirm each step, ensuring your images are properly prepared and stored for use in DreamBooth training.\n"]},{"cell_type":"code","execution_count":null,"id":"6f2ed033","metadata":{"collapsed":true,"id":"6f2ed033"},"outputs":[],"source":["# Name your concept (e.g., your dog's name)\n","concept_name = \"my-dog\" #@param {\"type\": \"string\"}\n","instance_data_dir = os.path.join(\"/content/\", concept_name)\n","\n","# Create the directory\n","os.makedirs(instance_data_dir, exist_ok=True)\n","\n","print(f\"Directory '{instance_data_dir}' created for your images. üí™\")\n","\n","# Check if the directory is empty before prompting for upload.\n","# If you already have images, I'm not going to make you keep re-uploading them.\n","if not os.listdir(instance_data_dir):\n","  print(f\"Uploading images to {instance_data_dir}‚Ä¶\")\n","  uploaded = files.upload(instance_data_dir)\n","else:\n","  print(f\"Directory '{instance_data_dir}' is not empty. Skipping image upload.\")\n","\n","output.clear()\n","print(\"ü©ª Images uploaded successfully.\")"]},{"cell_type":"markdown","source":["Next, we'll define two custom PyTorch Dataset classes‚Äî`DreamBoothDataset` and `PromptDataset`‚Äîwhich handle the data preparation for DreamBooth training.\n","\n","`DreamBoothDataset` loads and preprocesses your subject images (and optionally, class images for prior preservation), applies resizing and normalization transformations, and tokenizes prompts for use in model training. It ensures a balanced dataset when both instance and class images are used and returns processed image‚Äìprompt pairs for each training step.\n","\n","`PromptDataset` is a simpler utility that repeatedly provides a given text prompt for generating batches during image generation or testing.\n","\n","Together, these classes make it easy to feed properly formatted images and prompts into the DreamBooth model.\n"],"metadata":{"id":"-5V3SovponX0"},"id":"-5V3SovponX0"},{"cell_type":"code","source":["#@title Setup the Classes {\"display-mode\":\"form\"}\n","\n","# Import necessary libraries and modules\n","from pathlib import Path # Used for working with file paths in a platform-independent way\n","from torchvision import transforms # Provides common image transformations\n","\n","# Define a custom dataset class for DreamBooth training\n","class DreamBoothDataset(Dataset):\n","    # Initialize the dataset\n","    def __init__(\n","        self,\n","        instance_data_dir, # Directory containing the instance images (images of your subject)\n","        instance_prompt, # Prompt describing the instance images (e.g., \"a photo of sks dog\")\n","        tokenizer, # Tokenizer for processing text prompts\n","        class_data_root=None, # Optional directory for class images (for prior preservation)\n","        class_prompt=None, # Optional prompt describing the class images\n","        size=512, # The resolution to resize images to (e.g., 512x512)\n","        center_crop=False, # Whether to center crop images after resizing\n","    ):\n","        self.size = size # Store the image size\n","        self.center_crop = center_crop # Store the center crop setting\n","        self.tokenizer = tokenizer # Store the tokenizer\n","\n","        # Set up paths for instance images\n","        self.instance_data_dir = Path(instance_data_dir)\n","        if not self.instance_data_dir.exists():\n","            # Raise an error if the instance data directory doesn't exist\n","            raise ValueError(\"Instance images root doesn't exists.\")\n","\n","        # Get a list of all image paths in the instance directory\n","        self.instance_images_path = list(Path(instance_data_dir).iterdir())\n","        self.num_instance_images = len(self.instance_images_path) # Count the number of instance images\n","        self.instance_prompt = instance_prompt # Store the instance prompt\n","        self._length = self.num_instance_images # Initial dataset length based on instance images\n","\n","        # Set up paths for class images if prior preservation is used\n","        if class_data_root is not None:\n","            self.class_data_root = Path(class_data_root)\n","            self.class_data_root.mkdir(parents=True, exist_ok=True) # Create the class image directory if it doesn't exist\n","            self.class_images_path = list(Path(class_data_root).iterdir()) # Get a list of class image paths\n","            self.num_class_images = len(self.class_images_path) # Count the number of class images\n","            # Update dataset length to be the maximum of instance and class images for balancing\n","            self._length = max(self.num_class_images, self.num_instance_images)\n","            self.class_prompt = class_prompt # Store the class prompt\n","        else:\n","            self.class_data_root = None # Set class data root to None if not used\n","\n","        # Define the image transformations to apply to images\n","        self.image_transforms = transforms.Compose(\n","            [\n","                # Resize the image to the specified size using bilinear interpolation\n","                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n","                # Apply center crop or random crop based on the setting\n","                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n","                transforms.ToTensor(), # Convert the image to a PyTorch tensor\n","                # Normalize the image with mean 0.5 and standard deviation 0.5\n","                transforms.Normalize([0.5], [0.5]),\n","            ]\n","        )\n","\n","    # Return the length of the dataset\n","    def __len__(self):\n","        return self._length\n","\n","    # Get an item from the dataset at the given index\n","    def __getitem__(self, index):\n","        example = {} # Initialize a dictionary to store the example data\n","        # Load and process the instance image\n","        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n","        if not instance_image.mode == \"RGB\":\n","            instance_image = instance_image.convert(\"RGB\") # Convert to RGB if not already\n","        example[\"instance_images\"] = self.image_transforms(instance_image) # Apply transformations\n","\n","        # Tokenize the instance prompt\n","        example[\"instance_prompt_ids\"] = self.tokenizer(\n","            self.instance_prompt,\n","            padding=\"do_not_pad\",\n","            truncation=True,\n","            max_length=self.tokenizer.model_max_length,\n","        ).input_ids # Get the input IDs from the tokenized prompt\n","\n","        # If prior preservation is used, load and process a class image and tokenize the class prompt\n","        if self.class_data_root:\n","            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n","            if not class_image.mode == \"RGB\":\n","                class_image = class_image.convert(\"RGB\") # Convert to RGB if not already\n","            example[\"class_images\"] = self.image_transforms(class_image) # Apply transformations\n","            # Tokenize the class prompt\n","            example[\"class_prompt_ids\"] = self.tokenizer(\n","                self.class_prompt,\n","                padding=\"do_not_pad\",\n","                truncation=True,\n","                max_length=self.tokenizer.model_max_length,\n","            ).input_ids # Get the input IDs from the tokenized prompt\n","\n","        return example # Return the example dictionary\n","\n","\n","# Define a simple dataset for generating prompts\n","class PromptDataset(Dataset):\n","    # Initialize the dataset\n","    def __init__(self, prompt, num_samples):\n","        self.prompt = prompt # Store the prompt\n","        self.num_samples = num_samples # Store the number of samples to generate\n","\n","    # Return the length of the dataset\n","    def __len__(self):\n","        return self.num_samples\n","\n","    # Get an item from the dataset at the given index\n","    def __getitem__(self, index):\n","        example = {} # Initialize a dictionary to store the example data\n","        example[\"prompt\"] = self.prompt # Store the prompt in the example\n","        example[\"index\"] = index # Store the index in the example\n","        return example # Return the example dictionary"],"metadata":{"id":"JiBf1pSrEP-W"},"id":"JiBf1pSrEP-W","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generating the Class Images\n","\n","This code block ensures you have enough \"class images\" (generic images of your subject‚Äôs category, like other cats or dogs) to help prevent overfitting during DreamBooth training. If the target folder has fewer images than required, it loads a pre-trained Stable Diffusion model and automatically generates the missing images using a class prompt you provide. These images are saved to a specified directory and later used alongside your subject‚Äôs photos to maintain the model‚Äôs ability to generate diverse examples from the broader class, reducing the risk of catastrophic forgetting."],"metadata":{"id":"sAzKA5PopJzM"},"id":"sAzKA5PopJzM"},{"cell_type":"code","source":["#@title Generate Class Images {\"display-mode\":\"form\"}\n","\n","#@markdown `pretrained_model_name_or_path` which Stable Diffusion checkpoint you want to use\n","pretrained_model_name_or_path = \"stabilityai/stable-diffusion-2\" #@param [\"stabilityai/stable-diffusion-2\", \"stabilityai/stable-diffusion-2-base\", \"CompVis/stable-diffusion-v1-4\", \"runwayml/stable-diffusion-v1-5\"] {allow-input: true}\n","\n","#@markdown If the `prior_preservation_class_folder` is empty, images for the class will be generated with the class prompt. Otherwise, fill this folder with images of items on the same class as your concept (but not images of the concept itself)\n","prior_preservation_class_folder = \"./class_images\" #@param {type:\"string\"}\n","prior_preservation_class_prompt = \"a photo of a pit bull\" #@param {type:\"string\"}\n","\n","#@markdown `prior_preservation_weight` determins how strong the class for prior preservation should be\n","prior_loss_weight = 0.5 #@param {type: \"number\"}\n","\n","# Set the desired number of class images to generate\n","num_class_images = 12\n","\n","# Set the batch size for generating class images\n","sample_batch_size = 2\n","\n","\n","# Directory to save class images\n","class_images_dir = Path(prior_preservation_class_folder)\n","\n","# Create the directory if it doesn't exist\n","class_images_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Count existing class images\n","cur_class_images = len(list(class_images_dir.iterdir()))\n","\n","# Generate class images if the current number is less than the desired number\n","if cur_class_images < num_class_images:\n","    # Load the Stable Diffusion pipeline for image generation\n","    pipeline = StableDiffusionPipeline.from_pretrained(\n","        pretrained_model_name_or_path, revision=\"fp16\", torch_dtype=torch.float16\n","    ).to(\"cuda\")\n","    pipeline.enable_attention_slicing()\n","    pipeline.set_progress_bar_config(disable=True)\n","\n","    # Calculate the number of new images to generate\n","    num_new_images = num_class_images - cur_class_images\n","    print(f\"Number of class images to sample: {num_new_images}.\")\n","\n","    # Create a dataset and dataloader for generating images\n","    sample_dataset = PromptDataset(prior_preservation_class_prompt, num_new_images)\n","    sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=sample_batch_size)\n","\n","    # Generate and save the images\n","    for example in tqdm(sample_dataloader, desc=\"Generating class images\"):\n","        images = pipeline(example[\"prompt\"]).images\n","\n","        for i, image in enumerate(images):\n","            # Save the image to the class images directory\n","            image.save(class_images_dir / f\"{example['index'][i] + cur_class_images}.jpg\")\n","\n","    # Clean up the pipeline and free memory\n","    del pipeline\n","    gc.collect()\n","    with torch.no_grad():\n","      torch.cuda.empty_cache()\n","\n","# Update the number of class images after generation\n","num_class_images = len(list(class_images_dir.iterdir()))\n","\n","# Print the total number of class images\n","print(f\"Total number of class images: {num_class_images}\")"],"metadata":{"id":"3ZvEoU8kQP6v"},"id":"3ZvEoU8kQP6v","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we load ip the key pre-trained components of Stable Diffusion that DreamBooth training relies on:\n","\n","- the **CLIP tokenizer and text encoder** to process and embed text prompts,\n","- the **VAE (Variational Autoencoder)** to map images to and from a compact latent space for efficient training and generation,\n","- and the **U-Net model** that performs the core denoising steps to transform random noise into an image guided by the text embedding\n","\n","These modules form the backbone of Stable Diffusion, and DreamBooth fine-tunes primarily the U-Net while using the other components to condition and reconstruct images from your prompts.\n","\n","## U-Net\n","\n","The **U-Net model** is the part of the system that **actually shapes the noise in latent space into a meaningful image** based on your text prompt.\n","\n","Here‚Äôs how it works:\n","\n","1. **Starting Point ‚Äì Random Noise:** Generation begins with pure noise in the latent space (a chaotic mess of pixels, but compressed).\n","2. **U-Net‚Äôs Job ‚Äì Denoising:** The U-Net is a neural network trained to repeatedly look at this noise and figure out what should stay and what should be removed, gradually sculpting an image that matches your prompt.\n","3. **Guidance:** It uses information from the text encoder (your prompt turned into numbers) to decide how the image should evolve at each step.\n","\n","It‚Äôs called **U-Net** because of its **U-shaped architecture**:\n","\n","* It has a **downsampling path** (analyzes the noisy image at multiple scales, like zooming in and out).\n","* A **bottleneck** (understands global context).\n","* An **upsampling path** (rebuilds the image, refining it step by step).\n","\n","Imagine sculpting a statue from a rough block of stone. The U-Net is the sculptor ‚Äî it removes the random noise bit by bit until a detailed, prompt-matching image emerges in latent space.\n","\n","## Variational Autoencoder\n","\n","A **VAE (Variational Autoencoder)** is the component responsible for **translating images to and from latent space**:\n","\n","* **Encoder:** Takes a high-resolution image and compresses it into a **latent representation** (a much smaller, information-rich version of the image).\n","* **Decoder:** Takes that latent representation and reconstructs it back into a full-resolution image that humans can see.\n","\n","The reason this matters is efficiency:\n","\n","* Instead of doing heavy computations on millions of pixels, Stable Diffusion works in the compressed **latent space**, where generating an image is faster and requires less GPU memory.\n","* The VAE ensures that this compression and decompression keep the image realistic and detailed, even though it's heavily reduced internally.\n","\n","Think of the VAE as a **translator between human-readable images and the ‚Äúsecret shorthand language‚Äù (latent space)** that Stable Diffusion uses to dream up pictures.\n","\n"],"metadata":{"id":"MUJGaQvzrccq"},"id":"MUJGaQvzrccq"},{"cell_type":"code","source":["# @title Load the Model\n","\n","# Load the text encoder model from the pretrained model path and subfolder\n","text_encoder = CLIPTextModel.from_pretrained(\n","    pretrained_model_name_or_path, subfolder=\"text_encoder\"\n",")\n","\n","# Load the variational autoencoder (VAE) model from the pretrained model path and subfolder\n","vae = AutoencoderKL.from_pretrained(\n","    pretrained_model_name_or_path, subfolder=\"vae\"\n",")\n","\n","# Load the U-Net model from the pretrained model path and subfolder\n","unet = UNet2DConditionModel.from_pretrained(\n","    pretrained_model_name_or_path, subfolder=\"unet\"\n",")\n","\n","# Load the CLIP tokenizer from the pretrained model path and subfolder\n","tokenizer = CLIPTokenizer.from_pretrained(\n","    pretrained_model_name_or_path,\n","    subfolder=\"tokenizer\",\n",")"],"metadata":{"id":"guySZIp9S9a0"},"id":"guySZIp9S9a0","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Below, we define a **Namespace object** (`args`) that holds all the configuration settings needed to fine-tune Stable Diffusion with DreamBooth. It specifies key parameters such as which pre-trained model to start from, where to find your subject images and class images, the target image resolution, learning rate, number of training steps, batch sizes, and optimization details (like mixed precision, gradient accumulation, and 8-bit Adam). It also includes prior preservation options to prevent overfitting, seed values for reproducibility, and the output directory where checkpoints and the final trained model will be saved. These settings act as the blueprint for how your DreamBooth training session will run.\n"],"metadata":{"id":"MsEScyZIsg_N"},"id":"MsEScyZIsg_N"},{"cell_type":"code","source":["# @title Set Up All of the Training Parameters {\"display-mode\":\"form\"}\n","\n","#@title Setting up all training args\n","from argparse import Namespace\n","args = Namespace(\n","    # The name or path of the pretrained Stable Diffusion model to use.\n","    pretrained_model_name_or_path=pretrained_model_name_or_path,\n","    # The resolution for training and image generation.\n","    resolution=vae.sample_size,\n","    # Whether to center crop the images before resizing.\n","    center_crop=True,\n","    # Whether to train the text encoder. Training the text encoder can improve results but requires more memory.\n","    train_text_encoder=False,\n","    # The directory containing the instance images (images of your subject).\n","    instance_data_dir=instance_data_dir,\n","    # The prompt that describes your subject (e.g., \"a photo of sks dog\").\n","    instance_prompt=concept_name,\n","    # The learning rate for the optimizer.\n","    learning_rate=5e-06,\n","    # The total number of training steps.\n","    max_train_steps=300,\n","    # Save the model checkpoint every N steps.\n","    save_steps=50,\n","    # The batch size for training. Set to 1 when using prior preservation.\n","    train_batch_size=2,\n","    # Number of updates steps to accumulate before performing a backward/update pass.\n","    gradient_accumulation_steps=2,\n","    # Maximum gradient norm for gradient clipping.\n","    max_grad_norm=1.0,\n","    # Whether to use mixed precision training (\"fp16\" or \"bf16\"). \"fp16\" is recommended for faster training and lower memory usage.\n","    mixed_precision=\"fp16\",\n","    # Whether to use gradient checkpointing to save memory.\n","    gradient_checkpointing=True,\n","    # Whether to use 8-bit Adam optimizer from bitsandbytes for lower memory usage.\n","    use_8bit_adam=True,\n","    # The seed for reproducible training.\n","    seed=3434554,\n","    # Whether to use prior preservation loss. This helps prevent the model from overfitting to the instance images.\n","    with_prior_preservation=True,\n","    # The weight of the prior preservation loss.\n","    prior_loss_weight=prior_loss_weight,\n","    # The batch size for generating class images (used for prior preservation).\n","    sample_batch_size=2,\n","    # The directory containing the class images (images of the subject's class, but not the subject itself).\n","    class_data_dir=prior_preservation_class_folder,\n","    # The prompt that describes the class of your subject (e.g., \"a photo of a dog\").\n","    class_prompt=prior_preservation_class_prompt,\n","    # The number of class images to generate or use for prior preservation.\n","    num_class_images=num_class_images,\n","    # The learning rate scheduler to use.\n","    lr_scheduler=\"constant\",\n","    # The number of steps for the learning rate warmup.\n","    lr_warmup_steps=100,\n","    # The directory to save the trained model and checkpoints.\n","    output_dir=\"dreambooth-concept\",\n",")"],"metadata":{"id":"NkT-Kw7kTVM7"},"id":"NkT-Kw7kTVM7","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Defining the Training Function\n","\n","The **training\\_function**, which runs the full DreamBooth fine-tuning process for Stable Diffusion. It prepares the models, optimizer, data, and scheduler, then executes a multi-step training loop. The function freezes non-trainable components (like the VAE), sets up gradient accumulation and checkpointing to manage memory, and builds a dataset of subject and class images. During training, it repeatedly encodes images into latents, adds noise, predicts the noise using the U-Net (optionally with text encoder training), calculates loss (including prior preservation), backpropagates gradients, and updates model weights. Progress and losses are logged, checkpoints are saved at intervals, and the final fine-tuned pipeline is stored at the end.\n"],"metadata":{"id":"Qczvd7-ws0IZ"},"id":"Qczvd7-ws0IZ"},{"cell_type":"code","source":["# @title Training Function {\"display-mode\":\"form\"}\n","\n","from accelerate.utils import set_seed\n","import bitsandbytes as bnb # Import bitsandbytes with the alias bnb\n","import torch # Import torch here as well, just in case\n","\n","def training_function(text_encoder, vae, unet):\n","    # Get a logger for logging training progress\n","    logger = get_logger(__name__)\n","\n","    # Set the random seed for reproducibility\n","    set_seed(args.seed)\n","\n","    # Initialize Accelerator for distributed training and mixed precision\n","    accelerator = Accelerator(\n","        gradient_accumulation_steps=args.gradient_accumulation_steps,\n","        mixed_precision=args.mixed_precision,\n","    )\n","\n","    # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate\n","    # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.\n","    # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.\n","    if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:\n","        raise ValueError(\n","            \"Gradient accumulation is not supported when training the text encoder in distributed training. \"\n","            \"Please set gradient_accumulation_steps to 1. This feature will be supported in the future.\"\n","        )\n","\n","    # Freeze the VAE model as it's not being trained\n","    vae.requires_grad_(False)\n","    # Freeze the text encoder if not training it\n","    if not args.train_text_encoder:\n","        text_encoder.requires_grad_(False)\n","\n","    # Enable gradient checkpointing to save memory during training\n","    if args.gradient_checkpointing:\n","        unet.enable_gradient_checkpointing()\n","        if args.train_text_encoder:\n","            text_encoder.gradient_checkpointing_enable()\n","\n","    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n","    if args.use_8bit_adam:\n","        optimizer_class = bnb.optim.AdamW8bit\n","    else:\n","        optimizer_class = torch.optim.AdamW\n","\n","    # Determine which parameters to optimize (unet, or unet and text_encoder)\n","    params_to_optimize = (\n","        itertools.chain(unet.parameters(), text_encoder.parameters()) if args.train_text_encoder else unet.parameters()\n","    )\n","\n","    # Initialize the optimizer\n","    optimizer = optimizer_class(\n","        params_to_optimize,\n","        lr=args.learning_rate,\n","    )\n","\n","    # Load the noise scheduler from the pretrained model config\n","    noise_scheduler = DDPMScheduler.from_config(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n","\n","    # Ensure args.num_class_images is updated with the actual number of generated class images\n","    # Moved this update to before the DreamBoothDataset is initialized\n","    args.num_class_images = len(list(Path(args.class_data_dir).iterdir()))\n","\n","    # Create the DreamBooth dataset\n","    train_dataset = DreamBoothDataset(\n","        instance_data_dir=args.instance_data_dir,\n","        instance_prompt=args.instance_prompt,\n","        class_data_root=args.class_data_dir if args.with_prior_preservation else None,\n","        class_prompt=args.class_prompt,\n","        tokenizer=tokenizer,\n","        size=args.resolution,\n","        center_crop=args.center_crop,\n","    )\n","\n","    # Define the collation function for the dataloader\n","    def collate_fn(examples):\n","        # Extract input IDs and pixel values from examples\n","        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n","        pixel_values = [example[\"instance_images\"] for example in examples]\n","\n","        # concat class and instance examples for prior preservation\n","        if args.with_prior_preservation:\n","            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n","            pixel_values += [example[\"class_images\"] for example in examples]\n","\n","        # Stack pixel values into a single tensor\n","        pixel_values = torch.stack(pixel_values)\n","        # Convert pixel values to contiguous format and float type\n","        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n","\n","        # Pad input IDs to the maximum length\n","        input_ids = tokenizer.pad(\n","            {\"input_ids\": input_ids},\n","            padding=\"max_length\",\n","            return_tensors=\"pt\",\n","            max_length=tokenizer.model_max_length\n","        ).input_ids\n","\n","        # Create a batch dictionary\n","        batch = {\n","            \"input_ids\": input_ids,\n","            \"pixel_values\": pixel_values,\n","        }\n","        return batch\n","\n","    # Create the training dataloader\n","    train_dataloader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn\n","    )\n","\n","    # Initialize the learning rate scheduler\n","    lr_scheduler = get_scheduler(\n","        args.lr_scheduler,\n","        optimizer=optimizer,\n","        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n","        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n","    )\n","\n","    # Prepare models, optimizer, and dataloader for distributed training with Accelerator\n","    if args.train_text_encoder:\n","        unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n","            unet, text_encoder, optimizer, train_dataloader, lr_scheduler\n","        )\n","    else:\n","        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n","            unet, optimizer, train_dataloader, lr_scheduler\n","        )\n","\n","    # Determine the weight data type for mixed precision training\n","    weight_dtype = torch.float32\n","    if accelerator.mixed_precision == \"fp16\":\n","        weight_dtype = torch.float16\n","    elif accelerator.mixed_precision == \"bf16\":\n","        weight_dtype = torch.bfloat16\n","\n","    # Move text_encoder and vae to gpu.\n","    # For mixed precision training we cast the text_encoder and vae weights to half-precision\n","    # as these models are only used for inference, keeping weights in full precision is not required.\n","    vae.to(accelerator.device, dtype=weight_dtype)\n","    vae.decoder.to(\"cpu\") # Move VAE decoder to CPU to save GPU memory\n","    if not args.train_text_encoder:\n","        text_encoder.to(accelerator.device, dtype=weight_dtype)\n","\n","\n","    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n","    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n","    num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n","\n","    # Train!\n","    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n","\n","    # Log training information\n","    logger.info(\"***** Running training *****\")\n","    logger.info(f\"  Num examples = {len(train_dataset)}\")\n","    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n","    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n","    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n","    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n","    # Only show the progress bar once on each machine.\n","    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n","    progress_bar.set_description(\"Steps\")\n","    global_step = 0\n","\n","    # Start the training loop\n","    for epoch in range(num_train_epochs):\n","        # Set the unet (and text_encoder if training) to training mode\n","        unet.train()\n","        if args.train_text_encoder:\n","            text_encoder.train()\n","        # Iterate over the training dataloader\n","        for step, batch in enumerate(train_dataloader):\n","            # Accumulate gradients over gradient_accumulation_steps\n","            with accelerator.accumulate(unet):\n","                # Convert images to latent space using the VAE encoder\n","                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n","                # Scale latents by a constant factor\n","                latents = latents * 0.18215\n","\n","                # Sample noise that we'll add to the latents\n","                noise = torch.randn_like(latents)\n","                bsz = latents.shape[0]\n","                # Sample a random timestep for each image\n","                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n","                timesteps = timesteps.long()\n","\n","                # Add noise to the latents according to the noise magnitude at each timestep\n","                # (this is the forward diffusion process)\n","                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n","\n","                # Get the text embedding for conditioning from the text encoder\n","                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n","\n","                # Predict the noise residual using the U-Net model\n","                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n","\n","                # Get the target for loss depending on the prediction type\n","                if noise_scheduler.config.prediction_type == \"epsilon\":\n","                    target = noise # Target is the noise itself for epsilon prediction\n","                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n","                    target = noise_scheduler.get_velocity(latents, noise, timesteps) # Target is the velocity for v_prediction\n","                else:\n","                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n","\n","                # Compute loss, including prior preservation loss if enabled\n","                if args.with_prior_preservation:\n","                    # Chunk the noise and noise_pred into two parts and compute the loss on each part separately.\n","                    # The first half is for instance images, the second half is for class images.\n","                    noise_pred, noise_pred_prior = torch.chunk(noise_pred, 2, dim=0)\n","                    target, target_prior = torch.chunk(target, 2, dim=0)\n","\n","                    # Compute instance loss (loss on your subject's images)\n","                    loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\"none\").mean([1, 2, 3]).mean()\n","\n","                    # Compute prior loss (loss on the class images)\n","                    prior_loss = F.mse_loss(noise_pred_prior.float(), target_prior.float(), reduction=\"mean\")\n","\n","                    # Add the prior loss to the instance loss with a specified weight.\n","                    # This helps the model retain knowledge of the class while learning the instance.\n","                    loss = loss + args.prior_loss_weight * prior_loss\n","                else:\n","                    # If prior preservation is not used, the loss is just the MSE between predicted and target noise\n","                    loss = F.mse_loss(noise_pred.float(), target.float(), reduction=\"mean\")\n","\n","                # Perform backward pass to compute gradients\n","                accelerator.backward(loss)\n","\n","                # Clip gradients to prevent exploding gradients\n","                if accelerator.sync_gradients:\n","                    params_to_clip = (\n","                        itertools.chain(unet.parameters(), text_encoder.parameters())\n","                        if args.train_text_encoder\n","                        else unet.parameters()\n","                    )\n","                    accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)\n","                # Perform optimizer step to update model weights\n","                optimizer.step()\n","                # Zero out gradients after the optimizer step\n","                optimizer.zero_grad()\n","\n","            # Checks if the accelerator has performed an optimization step behind the scenes\n","            if accelerator.sync_gradients:\n","                # Update the progress bar and global step counter\n","                progress_bar.update(1)\n","                global_step += 1\n","\n","                # Save model checkpoint at specified intervals\n","                if global_step % args.save_steps == 0:\n","                    if accelerator.is_main_process:\n","                        # Create a StableDiffusionPipeline from the trained models\n","                        pipeline = StableDiffusionPipeline.from_pretrained(\n","                            args.pretrained_model_name_or_path,\n","                            unet=accelerator.unwrap_model(unet), # Unwrap the accelerated UNet model\n","                            text_encoder=accelerator.unwrap_model(text_encoder), # Unwrap the accelerated Text Encoder model\n","                        )\n","                        # Define the save path for the checkpoint\n","                        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n","                        # Save the pipeline\n","                        pipeline.save_pretrained(save_path)\n","\n","            # Log the current loss\n","            logs = {\"loss\": loss.detach().item()}\n","            # Update the progress bar with the current loss\n","            progress_bar.set_postfix(**logs)\n","\n","            # Break the loop if the maximum training steps are reached\n","            if global_step >= args.max_train_steps:\n","                break\n","\n","        # Wait for all processes to finish in distributed training\n","        accelerator.wait_for_everyone()\n","\n","    # Create the pipeline using using the trained modules and save it.\n","    if accelerator.is_main_process:\n","        # Create a StableDiffusionPipeline from the trained models\n","        pipeline = StableDiffusionPipeline.from_pretrained(\n","            args.pretrained_model_name_or_path,\n","            unet=accelerator.unwrap_model(unet), # Unwrap the accelerated UNet model\n","            text_encoder=accelerator.unwrap_model(text_encoder), # Unwrap the accelerated Text Encoder model\n","        )\n","        # Save the final trained pipeline\n","        pipeline.save_pretrained(args.output_dir)"],"metadata":{"id":"L0EcjEDQUgQc"},"id":"L0EcjEDQUgQc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run the Training Process\n","\n","Check whether **prior preservation** is enabled and ensures the required class images exist, generating any missing ones using a pre-trained Stable Diffusion pipeline and the provided class prompt. After confirming the dataset is complete, it prints diagnostic information, then calls `accelerate.notebook_launcher` to execute the `training_function`, which runs the actual fine-tuning of Stable Diffusion on your subject images. Once training finishes, it clears gradients and GPU memory to free up resources."],"metadata":{"id":"bd3x0wW3tJaS"},"id":"bd3x0wW3tJaS"},{"cell_type":"code","source":["#@title Run training\n","\n","import accelerate\n","import bitsandbytes as bnb # Import bitsandbytes with the alias bnb\n","import torch # Import torch here as well, just in case\n","from pathlib import Path\n","from diffusers import StableDiffusionPipeline # Import StableDiffusionPipeline for image generation\n","import os # Import os to list directory contents\n","\n","\n","# Ensure class images are generated before training\n","if args.with_prior_preservation:\n","    class_images_dir = Path(args.class_data_dir)\n","    if not class_images_dir.exists():\n","        class_images_dir.mkdir(parents=True)\n","\n","    cur_class_images = len(list(class_images_dir.iterdir()))\n","    if cur_class_images < args.num_class_images:\n","        pipeline = StableDiffusionPipeline.from_pretrained(\n","            args.pretrained_model_name_or_path, revision=\"fp16\", torch_dtype=torch.float16\n","        ).to(\"cuda\")\n","        pipeline.enable_attention_slicing()\n","        pipeline.set_progress_bar_config(disable=True)\n","\n","        num_new_images = args.num_class_images - cur_class_images\n","        print(f\"Number of class images to sample: {num_new_images}.\")\n","\n","        sample_dataset = PromptDataset(args.class_prompt, num_new_images)\n","        sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\n","\n","        for example in tqdm(sample_dataloader, desc=\"Generating class images\"):\n","            images = pipeline(example[\"prompt\"]).images\n","\n","            for i, image in enumerate(images):\n","                image.save(class_images_dir / f\"{example['index'][i] + cur_class_images}.jpg\")\n","        pipeline = None\n","        gc.collect()\n","        del pipeline\n","        with torch.no_grad():\n","          torch.cuda.empty_cache()\n","\n","    # Update args.num_class_images with the actual number of generated images\n","    args.num_class_images = len(list(class_images_dir.iterdir()))\n","    print(f\"Total number of class images: {args.num_class_images}\")\n","\n","# Add diagnostic print statements\n","print(f\"Contents of {args.class_data_dir}: {os.listdir(args.class_data_dir)}\")\n","print(f\"args.num_class_images before training: {args.num_class_images}\")\n","\n","\n","accelerate.notebook_launcher(training_function, args=(text_encoder, vae, unet))\n","for param in itertools.chain(unet.parameters(), text_encoder.parameters()):\n","  if param.grad is not None:\n","    del param.grad  # free some memory\n","  torch.cuda.empty_cache()"],"metadata":{"id":"6ItOqPv0UsAH"},"id":"6ItOqPv0UsAH","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"ec2b4e8b","metadata":{"id":"ec2b4e8b"},"source":["# Use the Fine-Tuned Model\n","\n","And now, we're going to set up the pipelines for the new fine-tuned model."]},{"cell_type":"code","execution_count":null,"id":"bc00e9ad","metadata":{"id":"bc00e9ad"},"outputs":[],"source":["from diffusers import DiffusionPipeline\n","\n","# Load the pipeline from the local output directory\n","model_path = args.output_dir # Use the output directory from the training step\n","pipe = DiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\")"]},{"cell_type":"markdown","id":"0b811450","metadata":{"id":"0b811450"},"source":["## Generate New Images\n","\n","The moment we've been waiting for. Use your trained model to create new images of your subject."]},{"cell_type":"code","execution_count":null,"id":"059f15d4","metadata":{"id":"059f15d4"},"outputs":[],"source":["suffix = \"in space\" # @param {\"type\":\"string\"}\n","# prompt = \"a photo of\" + concept_name + \" \" + suffix # Original line\n","prompt = f\"a photo of {concept_name} {suffix}\" # Using f-string for interpolation\n","\n","# Generate the image\n","image = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n","\n","# Display the image\n","image"]}],"metadata":{"colab":{"provenance":[],"gpuType":"A100","cell_execution_strategy":"setup","private_outputs":true,"toc_visible":true,"machine_shape":"hm"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}