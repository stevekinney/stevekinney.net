{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","private_outputs":true,"cell_execution_strategy":"setup","authorship_tag":"ABX9TyMSkf3uMpfOFDLcB28tWJ3J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from transformers import pipeline\n","from google.colab import output"],"metadata":{"id":"0BU6_xDxG8md"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Using Transformer Pipelines\n","\n","Think of the `pipeline` as a *magic wand*. It bundles together all the necessary steps to go from raw text to a structured, understandable output:\n","\n","1.  **Preprocessing:** Converts your text into numbers a model can understand (tokenization).\n","2.  **Model Inference:** Feeds the numbers through the model to get a prediction.\n","3.  **Post-processing:** Cleans up the model's output and presents it in a human-readable format."],"metadata":{"id":"AUMoCPsfPsEE"}},{"cell_type":"markdown","source":["We're going to play around with following types of pipelines.\n","\n","\n","| Task | Input | Output | Example Use Case |\n","|------|-------|--------|------------------|\n","| **Sentiment Analysis** | Text | Sentiment label + score | Product reviews |\n","| **Text Generation** | Prompt text | Generated continuation | Chatbots, story writing |\n","| **Zero-shot Classification** | Text + Categories | Category probabilities | Email routing |\n","| **Question Answering** | Question + Context | Extracted answer | FAQ systems |\n","| **Fill-Mask** | Text with `<mask>` | Predicted words | Autocomplete |\n","| **Summarization** | Long text | Short summary | News digests |\n","| **Tokenization** | Raw text | Tokens + IDs | All NLP tasks |\n","| **NER** | Text | Entity labels + positions | Privacy compliance |"],"metadata":{"id":"TQM7wDByS9Uq"}},{"cell_type":"markdown","source":["# Sentiment Analysis\n","\n","Let's start with a _real_ simple example. We'll use the sentiment analysis model to get the general gist of how positive or negative a given sentence is. Sentiment analysis is one of the most common NLP (Natural Language Processing) tasks. It determines whether a piece of text expresses positive or negative emotions. Think of it as teaching a computer to understand if someone is happy or upset based on their words.\n","\n","Sentiment analysis has numerous real-world applications including customer review analysis, social media monitoring, product feedback classification, and support ticket prioritization.\n","\n","Imagine you run an online store with thousands of reviews. Reading them all would be mind-numbingly boring. Sentiment analysis can:\n","\n","- Instantly categorize reviews as positive or negative\n","- Alert you to unhappy customers who need attention\n","- Track satisfaction trends over time\n","- Identify your best and worst products"],"metadata":{"id":"q8xOwUdrlmQ7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"l7vySU_9kjvz"},"outputs":[],"source":["sentiment_analyzer = pipeline(\n","    \"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",")\n","\n","texts = [\n","    \"I love this product! It's amazing.\",\n","    \"This is terrible. I hate it.\",\n","    \"It's okay, nothing special.\",\n","    \"This is bad and whoever made it should feel bad.\",\n","    \"Absolutely fantastic! Best purchase ever!\",\n","    \"Meh, could be better.\",\n","]\n","\n","\n","for text in texts:\n","    result = sentiment_analyzer(text)[0]\n","    label = result[\"label\"]\n","    confidence = result[\"score\"]\n","\n","    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n","    print(f\"Text: {text}\")\n","    print(f\"Sentiment: {label} (Confidence: {confidence:.3f})\")"]},{"cell_type":"markdown","source":["The confidence score tells you how sure the model is. A score of 0.999 means 99.9% confident (very sure), 0.750 means 75% confident (pretty sure), and 0.500 means 50% confident (uncertain).\n","\n"],"metadata":{"id":"QwEgSvjVNK2M"}},{"cell_type":"markdown","source":["`pipeline` is an abstraction that handles loading the model, tokenization (converting text to numbers), inference (making predictions), and post-processing (converting outputs to readable format).\n","\n","- **Pre-processing**: Using a tokenizer to pre-process the text and break it into tokens.\n","- **Processing**: Feeds the tokens to the model and gets the result.\n","- **Post-processing**: Gets us our feedback in the ways we might expect."],"metadata":{"id":"u_Amse6YoAjj"}},{"cell_type":"markdown","source":["## Common Pitfalls\n","\n","- **Binary Classification**: This default model only outputs `POSITIVE` or `NEGATIVE`. It doesn't have a `NEUTRAL` category.\n","- **Context Limitations**: The model might miss sarcasm or complex emotions.\n","- **Language**: The default model works best with English. For other languages, you'll need specific models.\n","\n","### Demonstrating a Pitfall: Neutrality and Sarcasm\n","\n","This model is trained for binary classification (`POSITIVE`/`NEGATIVE`) and can be easily confused by neutral statements or sarcasm."],"metadata":{"id":"qA363ZDzV1Jh"}},{"cell_type":"code","source":["neutral_text = \"This notebook is running on a server.\"\n","sarcastic_text = \"Oh, great. Another meeting. That's exactly what I needed.\"\n","\n","print(\"Analyzing a neutral sentence…\")\n","print(sentiment_analyzer(neutral_text))\n","\n","print(\"\\nAnalyzing a sarcastic sentence…\")\n","print(sentiment_analyzer(sarcastic_text))"],"metadata":{"id":"c3eHBXb2Qtxk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Text Generation\n","\n","Text generation is like having an AI co-writer. You provide a starting text (a \"prompt\"), and the model continues writing, mimicking the style and content it learned from a massive dataset of text. Unlike sentiment analysis, which *interprets* existing text, text generation *creates* new text.\n","\n","Unlike sentiment analysis—which analyzes existing text—generation creates new text that didn't exist before."],"metadata":{"id":"E03ypFfxq5_8"}},{"cell_type":"code","source":["from transformers import pipeline\n","\n","text_generator = pipeline(\"text-generation\", model=\"gpt2\")\n","\n","output.clear()\n","\n","prompts = [\n","    \"The future of artificial intelligence is\",\n","    \"Once upon a time in a distant galaxy\",\n","    \"The secret to happiness is\",\n","    \"In the year 2050, technology will\",\n","]\n","\n","for prompt in prompts:\n","    print(f\"{prompt}…\")\n","\n","    generated = text_generator(\n","        prompt,\n","        max_new_tokens=100,\n","        truncation=True,\n","        num_return_sequences=1,\n","        temperature=0.7,\n","        do_sample=True,\n","        pad_token_id=text_generator.tokenizer.eos_token_id,\n","    )\n","\n","    generated_text = generated[0][\"generated_text\"]\n","    print(generated_text)\n","    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n"],"metadata":{"id":"LT0v01xSomED"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice the difference from our sentiment analyzer:\n","\n","- We specify `\"text-generation\"` as the task\n","- We explicitly choose `model=\"gpt2\"`\n","\n","**Why specify the model?** Different models have different capabilities and sizes. GPT-2 is a good balance of quality and speed for learning. But, mostly we're impatient and we wanted a small, fast model.\n","\n","These prompts are intentionally:\n","\n","- **Open-ended**: They invite continuation rather than yes/no answers\n","- **Diverse**: From sci-fi to philosophy to predictions\n","- **Incomplete**: They end mid-thought, encouraging the model to continue\n","\n","## Understanding the Parameters\n","\n","- `max_length=100`\n","  - Maximum total length (prompt + generated text)\n","  - Prevents endless generation\n","  - Measured in \"tokens\" (roughly words/subwords)\n","- `num_return_sequences=1`\n","  - How many different versions to generate\n","  - Set to 2 or 3 to see different possibilities from the same prompt\n","- `temperature=0.7`: Think of temperature like a creativity dial. Low values (0.1-0.5) produce conservative, safe choices. Medium values (0.6-0.9) provide balanced creativity. High values (1.0+) result in experimental, risky choices.\n","  - Controls randomness/creativity (0.0 to 2.0)\n","  - **0.0**: Very predictable, repetitive\n","  - **0.7**: Balanced creativity (default)\n","  - **1.5+**: Wild, sometimes nonsensical\n","- `do_sample=True`\n","  - Enables probabilistic sampling\n","  - Without this, generation would be deterministic (same output every time)\n","\n","## Common Pitfalls\n","\n","- **Repetition**: Models can get stuck in loops. Adjust temperature or use repetition penalties.\n","- **Context Limits**: GPT-2 can only \"remember\" about 1024 tokens. Long prompts reduce generation space.\n","- **Inappropriate Content**: Models learn from internet text and may generate biased or inappropriate content.\n","- **Coherence Drift**: Longer generations may lose focus or contradict earlier statements.\n","\n","## Experimentation Ideas\n","\n","Try different temperatures (0.3, 0.7, and 1.2) to see how creativity changes. Use longer prompts to give more context for guiding generation. Set `num_return_sequences=3` to generate multiple versions. Compare different lengths with `max_length=50` versus `max_length=200`.\n","\n","You can also influence the generation style through your prompt:\n","\n","```python\n","# Formal style\n","\"In academic terms, artificial intelligence represents\"\n","\n","# Casual style\n","\"So basically, AI is like\"\n","\n","# Technical style\n","\"The implementation of neural networks requires\"\n","```"],"metadata":{"id":"hXu4fKWSNwQO"}},{"cell_type":"markdown","source":["# Classification\n","\n","Zero-shot classification is _almost_ magical: you can classify text into categories you define, without training the model on any examples. Unlike traditional machine learning (which needs thousands of labeled examples), zero-shot classification understands language well enough to match text to categories based on meaning alone.\n","\n","You can totally think of it as asking a smart assistant: \"Which of these categories best describes this text?\"\n","\n","\"Zero-shot\" means zero training examples. The model uses its understanding of language to connect texts with labels it's never seen before.\n","\n","## Tasting Notes\n","\n","- All scores sum to 1.0 (100%). The model distributes probability across all categories.\n","- The model actually creates a hypothesis (\"This text is about [category]\"), checks how well the hypothesis fits, compares all hypotheses, and returns probabilities.\n","\n","## How It Works\n","\n","The model converts classification into a natural language inference task:\n","\n","- For each category, it creates a hypothesis: \"This text is about [category]\"\n","- It evaluates how well each hypothesis fits the text\n","- It returns probability scores for all categories\n","\n","This approach is incredibly powerful because you can define new categories on the fly without retraining the model."],"metadata":{"id":"KT9quz3nsFVl"}},{"cell_type":"code","source":["classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n","\n","output.clear()\n","\n","candidate_labels = [\"business\", \"technology\", \"sports\", \"entertainment\", \"politics\"]\n","\n","texts = [\n","    \"Apple reported record quarterly earnings driven by strong iPhone sales.\",\n","    \"The new artificial intelligence model shows significant improvements in natural language understanding.\",\n","    \"The Denver Nuggets defeated the Lakers in last night's basketball game.\",\n","    \"The upcoming movie sequel has generated significant buzz among fans.\",\n","    \"The senator announced new legislation aimed at healthcare reform.\",\n","]\n","\n","for text in texts:\n","    result = classifier(text, candidate_labels)\n","\n","    print(f\"Text: {text}\")\n","    print(\"Classifications:\")\n","\n","    for label, score in zip(result[\"labels\"], result[\"scores\"]):\n","        print(f\"  {label}: {score:.3f}\")\n","\n","    print(\"\\n\" + \"-\" * 50 + \"\\n\")"],"metadata":{"id":"xslsRrjUsNQB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Common Pitfalls\n","\n","- **Ambiguous Labels** can cause confusion—\"positive\" could mean sentiment or medical test results, so be specific. **Too Many Categories** leads to lower confidence; start with 3-7 categories.\n","- **Similar Categories** like \"technology\" versus \"computers\" confuse the model. **Language Mismatch** between an English model and Spanish text produces poor results."],"metadata":{"id":"bId3Zw7nQruC"}},{"cell_type":"markdown","source":["## Practical Applications\n","\n","Zero-shot classification excels in **Customer Support** for routing tickets to departments, **Content Moderation** for flagging inappropriate content, **News Categorization** for organizing articles by topic, **Email Filtering** beyond simple spam detection, and **Sentiment Analysis** with custom emotion categories."],"metadata":{"id":"pAEGVlqpRgtj"}},{"cell_type":"markdown","source":["## Using Multiple Labels\n","\n","By default, the classifier assumes each text belongs to ONE category. We can allow multiple categories."],"metadata":{"id":"d4fbHe3BRXgP"}},{"cell_type":"code","source":["# Create classifier\n","classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n","\n","# Single-label classification\n","result = classifier(\n","    \"I love programming in Python!\",\n","    candidate_labels=[\"positive\", \"negative\", \"neutral\"]\n",")\n","\n","# Multi-label classification\n","result = classifier(\n","    \"The new iPhone is expensive but innovative.\",\n","    candidate_labels=[\"technology\", \"business\", \"review\"],\n","    multi_label=True\n",")\n","\n","print(result)"],"metadata":{"id":"_sQ0hiZ7RGok"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Try It Yourself\n","\n","Experiment with **Different Categories** like emotional states: `[\"happy\", \"sad\", \"angry\", \"surprised\"]`. Try **Longer Labels** using phrases: `[\"related to climate change\", \"about the economy\"]`. Test **Multi-Label** scenarios where a tech business story could be both \"technology\" AND \"business\". Explore **Non-English** labels in other languages, though results may vary."],"metadata":{"id":"NM-tiCAdRlym"}},{"cell_type":"markdown","source":["# Question Answering\n","\n","This is probably the one that most looks most reminiscent of ChatGPT, Claude, and Gemini if you squint closely—but not quite, as we'll see in a moment.. The one thing you'll notice that you're basically providing _all_ of it's context. So, it's basically only good at answering questions about the context that you have provided it.\n","\n","Question answering (QA) is like having a smart assistant read a document and answer your questions about it. The model finds the exact portion of text that answers your question—it doesn't generate new text, it extracts existing text.\n","\n","This is called _extractive_ QA because it extracts answers directly from the provided context.\n","\n","- **Extractive** (what we're using): Finds and extracts the exact text span that answers the question.\n","- **Generative** (a la ChatGPT): Creates new text to answer the question.\n","\n","Extractive QA is more reliable when you need precise information from source documents—it _never_ invents facts."],"metadata":{"id":"6l8pxU-v8V_2"}},{"cell_type":"code","source":["hotdog_historian = pipeline(\n","    \"question-answering\",\n","    model=\"distilbert-base-cased-distilled-squad\",\n",")\n","\n","context = \"\"\"\n","🌭 The modern hot dog traces its roots to German sausages brought to the U.S. by immigrants in the 1800s.\n","The first famous hot-dog stand popped up on Coney Island in 1871, allegedly selling 3,684 sausages in buns\n","during its first year. Classic toppings include mustard, ketchup (controversial in Chicago), relish, onions,\n","and sauerkraut. Nathan’s Famous launched its annual Hot Dog Eating Contest in 1916, turning competitive eating\n","into a Fourth-of-July spectacle.\n","\"\"\"\n","\n","questions = [\n","    \"Where did the first famous hot-dog stand appear?\",\n","    \"Which condiment causes drama in Chicago?\",\n","    \"What competitive eating event did Nathan’s Famous start?\",\n","]\n","\n","for question in questions:\n","    result = hotdog_historian(question=question, context=context)\n","\n","    print(f\"Question: {question}\")\n","    print(f\"Answer: {result['answer']}\")\n","    print(f\"Confidence: {result['score']:.3f}\")\n","    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n"],"metadata":{"id":"bOVYErJYuUt2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Simple and consistent with our previous pipelines. The pipeline automatically selects a suitable question-answering model (typically [DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert)-based) that's optimized for extractive QA tasks.\n","\n","The context is the \"document\" where the model looks for answers. The context must contain the answer since the model can't draw from general knowledge. Longer contexts are harder to process because there's more text to search through. Quality matters greatly—clear, well-written text produces better results."],"metadata":{"id":"L39DYwE9SZVZ"}},{"cell_type":"markdown","source":["## Key Concepts\n","\n","### Extractive vs. Generative\n","\n","**Extractive**—what we're using here—finds answers within provided text, always returns exact text from context, and can't answer if information isn't there. **Generative**—which is *actually* like ChatGPT—can combine information and reason, generates new text for answers, and can use general knowledge.\n","\n","### Confidence Scores\n","\n","Different from classification, QA confidence scores work as follows: high scores (&gt;0.9) indicate the model found a clear answer, medium scores (0.5-0.9) suggest an answer was found but the model is uncertain, and low scores (&lt;0.5) usually mean the model is probably guessing.\n","\n","### Answer Span\n","\n","The model actually predicts the start position of the answer, the end position of the answer, and then extracts the text between these positions."],"metadata":{"id":"My6jccGIS4x9"}},{"cell_type":"markdown","source":["Question answering transforms the challenge of information extraction into a simple Q&A interface. The model doesn't generate answers—it finds them in the text you provide. Success depends on asking clear questions and ensuring the answer actually exists in the context. High confidence scores (>0.9) usually indicate reliable answers, while low scores suggest the model is guessing. Remember: garbage in, garbage out—the quality of your context determines the quality of your answers."],"metadata":{"id":"sv-sXKy6TmLw"}},{"cell_type":"markdown","source":["## Common Pitfalls\n","\n","- **Answer Not in Context**: The model will guess, usually poorly. For example, with context \"The sky is blue\" and question \"What color is grass?\", it might return \"blue\" with low confidence.\n","- **Ambiguous Questions**: Multiple possible answers confuse the model.\n","- **Complex Reasoning**: The model can't handle \"Why?\" or \"How?\" questions well.\n","- **Long Contexts**: Performance degrades with very long documents."],"metadata":{"id":"lnVNdDqUTHPd"}},{"cell_type":"markdown","source":["### What Happens When the Answer Isn't There?\n","\n","The model is forced to find the *best possible* answer within the context, even if it's completely wrong. This is a critical limitation of extractive QA."],"metadata":{"id":"OqdGPibDRmM1"}},{"cell_type":"code","source":["no_answer_question = \"What is the best brand of mustard for a hot dog?\"\n","\n","result = hotdog_historian(question=no_answer_question, context=context)\n","\n","print(f\"Question: {no_answer_question}\")\n","print(f\"Answer: {result['answer']}\")\n","print(f\"Confidence: {result['score']:.3f}\")"],"metadata":{"id":"xQM5qaU7Ro7x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Notice**: The model returned \"Nathan’s Famous\" with a very low confidence score. This demonstrates that when the answer is not present, the model essentially makes a guess based on keyword proximity, and the low score is your signal that the answer is unreliable."],"metadata":{"id":"1cvKT5uORul1"}},{"cell_type":"markdown","source":["# Fill Mask\n","\n","[Fill-Mask](https://huggingface.co/tasks/fill-mask) is like a smart autocomplete that can predict missing words based on context. Fill-mask reveals how language models predict missing words. This is actually how [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) was trained—by learning to predict masked words in millions of sentences.\n","\n","Think of it as a sophisticated version of:\n","\n","> The cat sat on the `fill in the blank` → \"mat\"\n","\n","This is a fundamental task that BERT was trained on. While simpler than question answering, it provides insight into how language models learn word relationships and context.\n","\n","- Use masked language models to predict missing words\n","- Understand how BERT and similar models were trained\n","- Get multiple predictions with confidence scores\n","- Work with the special [MASK] token\n","\n","The `<mask>` token is special: exact spelling matters (must be lowercase in angle brackets for RoBERTa models), standard models handle only one mask at a time, and position matters since the model uses surrounding context.\n","\n","**Note**: Different models use different mask tokens—BERT uses `[MASK]` while RoBERTa uses `<mask>`. The pipeline handles this automatically.\n","\n","## Practical Applications\n","\n","Practical applications include text restoration (filling in damaged or censored text), data augmentation (generating training data variations), language learning (creating fill-in-the-blank exercises), autocomplete (smarter suggestions in text editors), and quality checking (testing if sentences make sense).\n","\n","### Why This Matters\n","\n","- **Shows model understanding**: See what the model \"thinks\" should come next\n","- **Reveals potential biases**: Discover what associations the model has learned\n","- **Demonstrates vocabulary**: Understand the model's word knowledge and limitations\n","\n","Fill-mask gives us a window into the model's internal representations and learned patterns."],"metadata":{"id":"NJKZZdcv7lKU"}},{"cell_type":"code","source":["fill_mask = pipeline(\"fill-mask\", model=\"distilroberta-base\")\n","\n","sentences = [\n","    \"The capital of France is <mask>.\",\n","    \"Python is a popular <mask> language.\",\n","    \"Machine learning is a subset of <mask> intelligence.\",\n","    \"The <mask> of gravity was discovered by Newton.\",\n","    \"Shakespeare wrote the play <mask>.\",\n","]\n","\n","\n","for sentence in sentences:\n","    print(f\"Original: {sentence}\")\n","\n","    results = fill_mask(sentence, top_k=3)\n","\n","    print(\"Top predictions:\")\n","    for i, result in enumerate(results, 1):\n","        filled_sentence = result[\"sequence\"]\n","        token = result[\"token_str\"]\n","        score = result[\"score\"]\n","        print(f\"  {i}. {filled_sentence} (token: '{token}', score: {score:.3f})\")\n","\n","    print(\"\\n\" + \"-\" * 50 + \"\\n\")"],"metadata":{"id":"IgpzloV-uhzF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Key Concepts\n","\n","### Masked Language Modeling (MLM)\n","\n","This is how BERT was trained: take a sentence (\"The dog chased the cat\"), randomly mask 15% of words (\"The <mask> chased the cat\"), train the model to predict \"dog\", and repeat billions of times.\n","\n","### Bidirectional Context\n","\n","Unlike GPT (left-to-right), BERT looks at both sides. For \"The <mask> barked loudly\", it uses both \"The\" and \"barked loudly\". This makes it better at understanding but worse at generation.\n","\n","### Token vs. Word\n","\n","Sometimes predictions include spaces: `token_str: \" Paris\"` (with leading space) or `token_str: \"Paris\"` (no space). The model handles subword tokens and spacing automatically."],"metadata":{"id":"p0yDRlnBUl5P"}},{"cell_type":"markdown","source":["## Common Pitfalls\n","\n","- **Wrong mask format**: Use `<mask>` for RoBERTa models, not `[mask]`, `{MASK}`, or `[MASK]`.\n","- **Multiple Masks**: Standard models struggle with multiple masks (e.g., `\"The <mask> <mask> is blue.\"`).\n","- **Out-of-Vocabulary Words**: The model might not know specialized terms.\n","- **Context Dependence**: Same mask, different contexts produce different results—\"Apple makes great <mask>\" → \"products\" while \"Apple is a delicious <mask>\" → \"fruit\".\n","\n","**Warning**: The predictions from fill-mask models reflect patterns in their training data, which can include societal biases. Always be aware of these limitations when deploying models in production, especially for sensitive applications."],"metadata":{"id":"GBnHOv7NUnS-"}},{"cell_type":"markdown","source":["# Summarization\n","\n","Summarization is the task of condensing long texts while preserving the most important information. It's like having an AI assistant read a long article and give you the key points. This is incredibly useful for processing large amounts of text quickly.\n","\n","There are two types: **extractive** (pulls out important sentences like highlighting) and **abstractive** (rewrites in new words like note-taking). Summarization condenses long texts while preserving key information. Modern models use **abstractive summarization**—they don't just extract sentences, they rewrite content concisely.\n","\n","Most Hugging Face models do abstractive summarization.\n","\n","- Use AI to summarize long texts automatically\n","- Control summary length with parameters\n","- Understand different summarization approaches\n","- Work with real-world text lengths\n","\n","This tutorial combines concepts from previous lessons - using pipelines to process text while controlling output with parameters similar to text generation.\n","\n","## Key Concepts\n","\n","- **Compression Ratio**: How much shorter the summary is compared to the original\n","- **Information Retention**: Which key facts are preserved in the summary\n","- **Readability**: How natural and coherent the summary sounds\n","\n","Good summarization balances brevity with completeness—keeping what matters while cutting what doesn't."],"metadata":{"id":"DzwDZfQ976Jt"}},{"cell_type":"markdown","source":["## Understanding the Parameters\n","\n","You can control the summary's length and style, much like with text generation:\n","\n","-   `max_length`: Sets the absolute maximum number of tokens in the final summary. The model will stop generating once it hits this limit.\n","-   `min_length`: Sets a minimum number of tokens. This forces the model to generate a summary that isn't trivially short.\n","-   `do_sample=False`: When `False` (the default for summarization), the model uses a deterministic approach, meaning it will produce the same summary every time for a given input. This is usually preferred for summarization to ensure consistency."],"metadata":{"id":"p4DLmIR7R6D5"}},{"cell_type":"code","source":["# prompt: Create an example that shows summarization.\n","\n","summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n","\n","text_to_summarize = \"\"\"\n","The modern hot dog traces its roots to German sausages brought to the U.S. by immigrants in the 1800s.\n","The first famous hot-dog stand popped up on Coney Island in 1871, allegedly selling 3,684 sausages in buns\n","during its first year. Classic toppings include mustard, ketchup (controversial in Chicago), relish, onions,\n","and sauerkraut. Nathan’s Famous launched its annual Hot Dog Eating Contest in 1916, turning competitive eating\n","into a Fourth-of-July spectacle.\n","\"\"\"\n","\n","summary = summarizer(text_to_summarize, max_length=50, min_length=10, do_sample=False)\n","\n","print(\"Original Text:\")\n","print(text_to_summarize)\n","print(\"\\nSummary:\")\n","print(summary[0]['summary_text'])"],"metadata":{"id":"Rq8mhqPjxIo5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Key Concepts\n","\n","### Compression Ratio\n","\n","Original text to summary represents compression. For example, 1000 chars to 200 chars equals 5:1 compression. Higher compression means more information loss, so balance brevity with completeness.\n","\n","### Token Limits\n","\n","Models have maximum context windows and can't summarize extremely long documents in one pass. You may need to chunk very long texts, with typical limits being 512-1024 tokens.\n","\n","### Quality vs Length Trade-off\n","\n","Shorter summaries provide key facts only, while longer summaries include more detail and nuance. Find the sweet spot for your use case."],"metadata":{"id":"4NjICc2kVYnN"}},{"cell_type":"markdown","source":["## Common Pitfalls\n","\n","- **Text Too Short**: Summarizing already-brief text produces poor results.\n","- **Length Mismatch**: Don't set max_length longer than the input text (e.g., `summarizer(\"Hello world\", max_length=100)`).\n","- **Information Loss**: Critical details might be omitted.\n","- **Factual Errors**: Models can sometimes generate incorrect information."],"metadata":{"id":"GeQqI2ZMVdoB"}},{"cell_type":"markdown","source":["# Named Entity Recognition\n","\n","Let's talk about how to extract named entities (people, organizations, locations) from text, understand entity types and confidence scores, use aggregation strategies for better results, and highlight entities within text programmatically.\n","\n","[Named Entity Recognition (NER)](https://en.wikipedia.org/wiki/Named-entity_recognition) is the task of identifying and classifying named entities in text. It's like having an AI highlight all the important names and places in a document. This is crucial for information extraction, document analysis, building knowledge graphs, and privacy/compliance (finding personal information). Named Entity Recognition (NER) identifies and classifies named entities in text—people, organizations, locations, and more. It's like having a smart highlighter that automatically marks important names and places.\n","\n","Standard NER models recognize **PER** (Person: names of people), **ORG** (Organization: companies, agencies, institutions), **LOC** (Location: countries, cities, addresses), and **MISC** (Miscellaneous: everything else like dates, events, etc.).\n","\n","Different from previous tasks, NER confidence scores work as follows: high confidence (&gt;0.9) indicates a clear entity, medium (0.7-0.9) suggests a probable entity, and low (&lt;0.7) means uncertain and might be wrong."],"metadata":{"id":"YmHlRBVabPPs"}},{"cell_type":"code","source":["ner_pipeline = pipeline(\n","    \"ner\",\n","    aggregation_strategy=\"simple\",\n","    model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",")\n","\n","texts = [\n","    \"My name is John Smith and I work at Apple Inc. in Cupertino, California.\",\n","    \"Barack Obama was the 44th President of the United States from 2009 to 2017.\",\n","    \"Microsoft was founded by Bill Gates and Paul Allen in Albuquerque, New Mexico.\",\n","    \"The meeting is scheduled for December 15th at Google headquarters in Mountain View.\",\n","    \"Tesla CEO Elon Musk announced the new factory will be built in Austin, Texas.\",\n","]\n","\n","for text in texts:\n","    print(f\"Text: {text}\")\n","\n","    entities = ner_pipeline(text)\n","\n","    if entities:\n","        print(\"Entities found:\")\n","        for entity in entities:\n","            word = entity[\"word\"]\n","            label = entity[\"entity_group\"]\n","            confidence = entity[\"score\"]\n","            start = entity[\"start\"]\n","            end = entity[\"end\"]\n","\n","            print(\n","                f\"  '{word}' -> {label} (confidence: {confidence:.3f}, position: {start}-{end})\"\n","            )\n","    else:\n","        print(\"  No entities found\")\n","\n","    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n","\n","sample_text = \"Apple CEO Tim Cook visited the Tesla factory in Berlin, Germany.\"\n","entities = ner_pipeline(sample_text)\n","\n","highlighted_text = sample_text\n","offset = 0\n","\n","for entity in sorted(entities, key=lambda x: x[\"start\"]):\n","    start = entity[\"start\"] + offset\n","    end = entity[\"end\"] + offset\n","    word = entity[\"word\"]\n","    label = entity[\"entity_group\"]\n","\n","    replacement = f\"[{word}:{label}]\"\n","    highlighted_text = highlighted_text[:start] + replacement + highlighted_text[end:]\n","    offset += len(replacement) - len(word)\n","\n","print(f\"Original: {sample_text}\")\n","print(f\"Highlighted: {highlighted_text}\")\n"],"metadata":{"collapsed":true,"id":"sW73rcazbY3M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Common Pitfalls\n","\n","- **Missing Aggregation**: Always use `aggregation_strategy=\"simple\"` to get complete words instead of subword tokens.\n","- **Case Sensitivity**: Some models are case-sensitive—\"apple\" (fruit) versus \"Apple\" (company).\n","- **Context Matters**: \"Jordan\" could be person or country, \"Washington\" could be person, city, or state.\n","- **Overlapping Entities**: Standard models don't handle overlaps well."],"metadata":{"id":"B73ZeNbScPGO"}},{"cell_type":"markdown","source":["## Entity Type Reference\n","\n","NER uses the **IOB** (Inside-Outside-Beginning) tagging scheme:\n","\n","- **B-XXX**: Beginning of entity type XXX\n","- **I-XXX**: Inside (continuation) of entity type XXX  \n","- **O**: Outside any entity\n","\n","This scheme allows models to handle multi-word entities correctly—for example, \"New York City\" is tagged as [B-LOC, I-LOC, I-LOC].\n","\n","| Type       | Description                      | Examples                          |\n","| ---------- | -------------------------------- | --------------------------------- |\n","| PER/PERSON | People, characters               | \"Albert Einstein\", \"Harry Potter\" |\n","| ORG        | Organizations                    | \"NASA\", \"United Nations\"          |\n","| LOC/GPE    | Locations, geopolitical entities | \"Paris\", \"Pacific Ocean\"          |\n","| DATE       | Dates and periods                | \"January 2024\", \"last week\"       |\n","| TIME       | Times                            | \"3:30 PM\", \"morning\"              |\n","| MONEY      | Monetary values                  | \"$100\", \"50 euros\"                |\n","| PERCENT    | Percentages                      | \"50%\", \"half\"                     |\n","| MISC       | Miscellaneous                    | Events, products, etc.            |"],"metadata":{"id":"pZqoCG7PcX51"}},{"cell_type":"markdown","source":["# Translation\n","\n","Translation is another core NLP task where `pipeline` shines. You can translate text between numerous languages by simply loading the correct model.\n","\n","Models are typically named with the source and target languages in mind. For this example, we'll use a model from the Helsinki-NLP group, which has published many high-quality translation models."],"metadata":{"id":"hBVNUqMeSBRA"}},{"cell_type":"code","source":["translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")\n","\n","text = \"The future of artificial intelligence is fascinating.\"\n","\n","translation = translator(text)\n","\n","print(f\"Original (English): {text}\")\n","print(f\"Translated (German): {translation[0]['translation_text']}\")"],"metadata":{"id":"zNeCAviMSEDj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Conclusion & Next Steps\n","\n","Right on. You've successfully used the Hugging Face `pipeline` to perform seven different NLP tasks:\n","\n","1.  **Sentiment Analysis**: Classifying text as positive or negative.\n","2.  **Text Generation**: Creating new text from a prompt.\n","3.  **Zero-Shot Classification**: Categorizing text with custom labels on the fly.\n","4.  **Question Answering**: Extracting answers from a given context.\n","5.  **Fill-Mask**: Predicting missing words in a sentence.\n","6.  **Summarization**: Condensing long documents into key points.\n","7.  **Named Entity Recognition (NER)**: Identifying people, places, and organizations.\n","8.  **(Bonus) Translation**: Translating text between languages.\n","\n","Definitely explore the [Hugging Face Hub](https://huggingface.co/models) to find thousands of other pre-trained models for any task imaginable."],"metadata":{"id":"TapU2X_rSL28"}}]}