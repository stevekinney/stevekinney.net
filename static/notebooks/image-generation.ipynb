{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://gist.github.com/stevekinney/bfdfc4289148921be05a119cfae7edd3#file-image-generation-ipynb","timestamp":1753570854945}],"machine_shape":"hm","gpuType":"T4","cell_execution_strategy":"setup","private_outputs":true,"authorship_tag":"ABX9TyPFwVlWBs8wTvLkucQ5rrZ7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Generating Images from Text\n","\n","[Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion) and similar models can create stunning images from text descriptions, revolutionizing creative workflows. Unlike classification models that analyze existing images, diffusion models create new images from noise.\n","\n","Stable Diffusion is like a super-smart magic coloring book: you whisper what picture you want—say, “a purple dinosaur eating ice cream on the moon”—and the coloring book starts with a page covered in silly scribbles (just random dots). Then, almost like an eraser and crayon working together, it keeps gently wiping away the messy dots and adding the right colors and shapes bit by bit, listening to your words the whole time, until the random mess turns into the exact picture you asked for."],"metadata":{"id":"W6wjBnMMg6RB"}},{"cell_type":"code","source":["# @title Install the Dependencies {\"display-mode\":\"form\"}\n","\n","# Install and import dependencies\n","!pip install --quiet \"diffusers[torch]\" transformers accelerate safetensors\n","\n","import torch\n","from diffusers import AutoPipelineForText2Image, DPMSolverMultistepScheduler\n","from google.colab import output\n","\n","# Set up some basic variables.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","generator = torch.Generator(device).manual_seed(1337)"],"metadata":{"id":"zSCQ5KFe1hgz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Generating an Image\n","\n","model_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n","\n","pipeline = AutoPipelineForText2Image.from_pretrained(\n","  model_id,\n","  torch_dtype=torch.float16,\n","  use_safetensors=True,\n","  low_cpu_mem_usage=True\n",").to(device)\n","\n","# Swap to a faster, lower‑VRAM scheduler\n","pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n","\n","# Memory‑saving toggles (≈‑1.5 GB on a T4)\n","pipeline.enable_attention_slicing()                  # chunk cross‑attention\n","pipeline.enable_vae_slicing()                        # same for VAE decoder\n","pipeline.enable_model_cpu_offload()                  # swap idle layers to CPU\n","\n","prompt = \"A pygmy hippo in New York City watching Instagram on an iPhone\" # @param {\"type\":\"string\",\"placeholder\":\"Prompt\"}\n","\n","image = pipeline(\n","    prompt,\n","    num_inference_steps=50,\n","    guidance_scale=7.5,\n","    generator=generator,\n",").images[0]\n","\n","output.clear()\n","\n","image"],"metadata":{"id":"IzcMXSbShxsq","cellView":"code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Quality versus Speed\n","\n","You can tweak some knobs to control whether you want your image to be good or if you're in a hurry.\n","\n","-\t`num_inference_steps`: 10–50, lower is faster.\n","-\t`guidance_scale`: 1–15 Higher if going to be more faithful to your text, but also a lot stiffer.\n","-\t`width / height`: Stick to multiples of 64; go larger only if your VRAM's been hitting the gym.\n","\n","## CUDA or CPU\n","\n","**TL;DR**: Do we have a GPU or not.\n","\n","```py\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","```\n","\n","That one‑liner is a quick “gear check” for PyTorch: it asks, *“Do I have an NVIDIA GPU with CUDA drivers ready?”*—that’s what `torch.cuda.is_available()` returns. If the answer is **yes**, the variable `device` is set to the string `\"cuda\"`, which tells PyTorch to park tensors and run math on the GPU (turbo‑charged parallel cores). If the answer is **no**—maybe you’re on a laptop with integrated graphics—the expression falls back to `\"cpu\"`, so everything runs on the regular processor instead. In short, it’s a portable toggle that automatically picks the fastest hardware you actually have, no manual edits required.\n","\n","## What is a generator?\n","\n","Defining a generator sets up a private dice-roller for PyTorch that always starts at the same spot:\n","\n","1. `torch.Generator(device)` → creates a random‑number generator that lives on your GPU (or CPU) instead of using the global one.\n","2. `.manual_seed(1337)` → rewinds that generator to the exact same “first roll” every run.\n","\n","Pass this `generator` to Diffusers and the noise it injects—and therefore the final image—will be identical as long as everything else is unchanged. Drop the seed or reuse the same generator for multiple calls, and the randomness (and thus your pictures) will start diverging.\n","\n"],"metadata":{"id":"891o5WwtnnLb"}},{"cell_type":"markdown","source":["## What Are Diffusion Models?\n","\n","Diffusion models are a class of generative AI models that create images by gradually removing noise from random patterns. Think of it like this:\n","\n","1. **Forward Process**: Take a clear image and gradually add noise until it becomes pure random noise\n","2. **Reverse Process**: Train a model to reverse this process - start with noise and gradually remove it to create an image\n","\n","### Key Components\n","\n","1. **U-Net**: The neural network that predicts and removes noise\n","2. **Text Encoder**: Converts your text prompt into embeddings the model understands\n","3. **Scheduler**: Controls how noise is added/removed over multiple steps\n","4. **VAE (Variational Autoencoder)**: Compresses images to/from latent space for efficiency"],"metadata":{"id":"um5hZg35kx8X"}},{"cell_type":"markdown","source":["# Prompt Engineering\n","\n","The quality of your generated images heavily depends on how you write your prompts. Let's talk about some best practices.\n","\n","### Prompt Structure\n","\n","A good prompt typically includes:\n","1. **Subject**: What/who is in the image\n","2. **Style**: Artistic style, medium, or technique\n","3. **Quality modifiers**: Words that enhance quality\n","4. **Lighting**: How the scene is lit\n","5. **Composition**: Camera angle, framing\n","6. **Details**: Specific attributes or characteristics"],"metadata":{"id":"jQJgvMrDmOo4"}},{"cell_type":"markdown","source":["### Negative Prompts\n","\n","**Negative prompts** are simply “don’t-do-this” instructions you feed alongside your regular text prompt. In a Stable Diffusion pipeline that uses *classifier-free guidance* (CFG), the model always denoises the image twice at every step—once with your positive prompt and once with an *un-conditioned* prompt (normally an empty string). The final update is:\n","\n","$$\n","\\text{predicted_noise}= \\text{uncond} + s \\,(\\text{cond}-\\text{uncond}),\n","$$\n","\n","*s* is the guidance scale. If you replace that empty un-conditioned prompt with a sentence listing unwanted features (e.g. “blurry, extra fingers, watermark”), you tilt the subtraction term so the model actively steers away from those concepts: the more they would appear, the stronger the vector pointing in the opposite direction. Practically, this lets you ban artifacts (like text or low-res edges), rein in style drift, or remove whole objects without retraining, because the negative prompt injects a *repulsive* semantic direction into every denoising step. Empirically and in recent analyses, this “hijacked un-cond branch” explanation matches what we see: adding “worst quality, jpeg artifacts” to the negative prompt reliably suppresses those flaws, while research papers show the latent-space cancellation effect that deletes the specified concepts. (Sources: [Stable Diffusion Art][1], [arXiv][2])\n","\n","[1]: https://stable-diffusion-art.com/how-negative-prompt-work/?utm_source=chatgpt.com \"How does negative prompt work? - Stable Diffusion Art\"\n","[2]: https://arxiv.org/html/2406.02965v1?utm_source=chatgpt.com \"Understanding the Impact of Negative Prompts: When and How Do ...\"\n"],"metadata":{"id":"tFM0t7JFovcx"}},{"cell_type":"code","source":["# @title Using Negative Prompts {\"display-mode\":\"both\"}\n","model_id = \"stabilityai/stable-diffusion-xl-bse-1.0\" # @param [\"runwayml/stable-diffusion-v1-5\",\"stabilityai/stable-diffusion-2-1\",\"stabilityai/stable-diffusion-xl-bse-1.0\",\"CompVis/stable-diffusion-v1-4\",\"prompthero/openjourney\",\"hakurei/waifu-diffusion\",\"nitrosocke/Ghibli-Diffusion\"]\n","\n","\n","pipeline = AutoPipelineForText2Image.from_pretrained(\n","    model_id,\n","    torch_dtype=torch.float16,\n","    use_safetensors=True,\n",").to(device)\n","\n","# drop‑in scheduler swap (no retraining needed)\n","pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n","\n","pipeline.enable_attention_slicing()   # VRAM saver\n","\n","# EXEMPLAR (positive) prompt — subject → environment → style → camera/lens → lighting\n","prompt = (\n","    \"Ultra‑realistic cinematic photo of a pygmy hippo jay‑walking through a neon‑lit \"\n","    \"downtown street at dusk, rain‑soaked asphalt reflecting colored lights, 85 mm lens, \"\n","    \"f/1.4 bokeh, dramatic rim lighting, shot on Kodak Portra 800, 8‑K resolution\"\n",")\n","\n","# NEGATIVE prompt — ban common artifacts & unwanted elements\n","negative_prompt = (\n","    \"blurry, grainy, lowres, overexposed, watermark, text, logo, extra limbs, cars, people, \"\n","    \"distorted anatomy, jpeg artifacts\"\n",")\n","\n","width = 512 # @param {\"type\":\"number\"}\n","height = 512 # @param {\"type\":\"number\"}\n","\n","image = pipeline(\n","    prompt=prompt,\n","    negative_prompt=negative_prompt,\n","    num_inference_steps=28,\n","    guidance_scale=7.0,\n","    width=width,\n","    height=height,\n","    generator=generator,\n",").images[0]\n","\n","output.clear()\n","\n","image"],"metadata":{"id":"aRCNfWEwqF1D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Explanation\n","\n","## `DPMSolverMultistep`\n","\n","Think of your picture-painting robot (the `pipe`) as having a little instruction booklet that tells it exactly how to wipe away the foggy noise and reveal the picture. That line of code is like saying, “Hey robot, swap your old booklet for this *new* booklet called `DPMSolverMultistep`, but copy over all the page numbers so you still know when to start and stop each wipe.” So the robot keeps the same schedule (how many wipes, when they happen) but uses a smarter cleaning style that can make the picture look nicer or appear faster.\n","\n","\n","## Enable Attention Slicing\n","\n","Imagine you have a huge coloring book page that's too big to fit on your little table. Instead of trying to color the whole giant page at once, you fold it into smaller squares and color one square at a time, then unfold it to see the full picture—easy and no mess. `pipe.enable_attention_slicing()` tells the computer to do the same thing with its “brain power” when making an image: it looks at one small chunk of the picture at a time instead of everything all at once, so it doesn't run out of room (memory) and can still finish the whole picture nicely.\n","\n","## Why though?\n","\n","In short, attention slicing keeps your GPU from running out of memory, while the DPM++ scheduler squeezes more quality (or speed) out of each denoising step. Together they make the pipeline both lighter and sharper without touching the model weights or your prompt."],"metadata":{"id":"7bjklkpQsJk5"}},{"cell_type":"markdown","source":["# Other Models to Experiment With\n","\n","- `runwayml/stable-diffusion-v1-5`: Classic SD 1.5—Fast, versatile\n","- `stabilityai/stable-diffusion-2-1`: SD 2.1—Better faces, higher res\n","- `stabilityai/stable-diffusion-xl-bse-1.0`: SDXL—Highest quality, slower\n","- `CompVis/stable-diffusion-v1-4`: Original SD 1.4\n","- `prompthero/openjourney`: Midjourney style\n","- `hakurei/waifu-diffusion`: Anime style\n","- `nitrosocke/Ghibli-Diffusion`: Studio Ghibli style"],"metadata":{"id":"12xdtTT3qbVy"}}]}