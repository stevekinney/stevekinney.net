{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyOUR4VL0SvetUKX9RBumOcD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Tokenization\n","\n","Tokenizers are the bridge between human text and AI models. Since neural networks only understand numbers, we need to convert text into numerical representations. This process involves splitting text into smaller pieces (tokens), converting tokens to numbers (IDs), and adding special markers for the model.\n","\n","Tokenization converts human-readable text into numbers that models can process:\n","\n","1. **Split** text into smaller units (tokens)\n","2. **Map** tokens to unique IDs from a vocabulary\n","3. **Add** special tokens that give the model instructions\n","4. **Create** attention masks to handle batches efficiently"],"metadata":{"id":"-z4oX_9bWcW8"}},{"cell_type":"markdown","source":["# Settings Up Our Tokenizers\n","\n","First, let's load a few different tokenizers from the transformers library. Different models use different tokenization strategies, and it's important to see how they compare. We'll look at three of the most common ones: **BERT**, **RoBERTa**, **GPT-2**, and **T5**."],"metadata":{"id":"m21RzBcOVCZu"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","from google.colab import output\n","\n","# Load tokenizers for different models\n","tokenizers = {\n","    \"BERT\": AutoTokenizer.from_pretrained(\"bert-base-uncased\"),\n","    \"GPT-2\": AutoTokenizer.from_pretrained(\"gpt2\"),\n","    \"RoBERTa\": AutoTokenizer.from_pretrained(\"roberta-base\"),\n","    \"T5\": AutoTokenizer.from_pretrained(\"t5-small\")\n","}\n","\n","output.clear() # Clears out all of those fun progress bars.\n","\n","print(\"✅ Loaded tokenizers for comparison:\")\n","for name, tokenizer in tokenizers.items():\n","    print(f\"  - {name}: Vocabulary size = {tokenizer.vocab_size:,}\")"],"metadata":{"id":"f6DxxbEBW0xa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Vocabulary\n","\n","Each tokenizer has a fixed vocabulary size. BERT, for instance, knows about 30,000 unique tokens, while GPT-2 knows about 50,000. Any word or character not in this vocabulary will be broken down further or marked as unknown."],"metadata":{"id":"6S1cq5X2dc9L"}},{"cell_type":"markdown","source":["# Basic Tokenization\n","\n","The most basic step is splitting a piece of text into a list of tokens. For BERT, common words like \"hello\" become a single token, but less common words like \"tokenization\" are often broken into smaller subwords (`token` and `##ization`).\n","\n","The `##` prefix in BERT's tokenizer signifies that the token is a continuation of the previous one. Let's see this in action.\n","\n","`AutoTokenizer` automatically loads the right tokenizer type, `\"bert-base-uncased\"` means lowercase BERT tokenizer, and each model has its own tokenizer—they're not interchangeable."],"metadata":{"id":"ec2psmc6Xvqp"}},{"cell_type":"code","source":["text = \"Hello, world! Tokenization is the process of converting text into tokens.\" # @param {\"type\":\"string\",\"placeholder\":\"Sample Text\"}\n","tokenizer_id = \"BERT\" # @param [\"BERT\",\"GPT-2\",\"T5\",\"RoBERTa\"]\n","tokenizer = tokenizers[tokenizer_id]\n","tokens = tokenizer.tokenize(text)\n","\n","print(tokens)"],"metadata":{"id":"WBg8MsYhX1W2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Try this**: Modify the text variable in the cell above and rerun it. Try using a sentence with more complex or unusual words to see how the tokenizers handle it. Also, try out some different tokenizers and compare the results."],"metadata":{"id":"R4t3c545Vrpg"}},{"cell_type":"markdown","source":["# From Tokens to IDs: Preparing for the Model\n","\n","Now that we have tokens, we need to convert them into numbers that the model can actually use. Each token in a tokenizer's vocabulary has a unique ID.\n","\n","The `.encode()` method handles both tokenizing and converting to IDs in one step. It also adds the special tokens that the model needs to understand the sequence's structure."],"metadata":{"id":"n86uffSOYVPb"}},{"cell_type":"code","source":["# Let's use the BERT tokenizer as our main example\n","tokenizer = tokenizers[\"BERT\"]\n","text = \"Hello, world!\"\n","\n","# The encode method adds special tokens and converts to IDs\n","token_ids = tokenizer.encode(text)\n","\n","print(f\"Original Text: {text}\")\n","print(f\"Token IDs: {token_ids}\")"],"metadata":{"id":"wEJ2juC-YgpY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Each token has a unique ID: \"hello\" → 7592, \"world\" → 2088. Numbers are what the model actually processes."],"metadata":{"id":"O_0HfWc9YvMj"}},{"cell_type":"markdown","source":["# Decoding Back to Text and Special Tokens\n","\n","This reverses the process (IDs → tokens → text) and is useful for understanding model outputs. This reverses the process (IDs → tokens → text) and is useful for understanding model outputs.\n","\n","BERT adds special tokens: **[CLS]** (101) for start of sequence used for classification, **[SEP]** (102) as separator between sentences, and **[PAD]** (0) for padding during batch processing.\n","\n","Example: \"Hello\" becomes [101, 7592, 102] → [CLS] hello [SEP]"],"metadata":{"id":"GkH9a2AVZC8Z"}},{"cell_type":"code","source":["decoded = tokenizer.decode(token_ids)\n","\n","print(\"Decoded:\", decoded)"],"metadata":{"id":"2Lmi6nJmZHbz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Special tokens are crucial for model performance:\n","\n","- `[CLS]`: Marks the beginning of input (used for classification)\n","- `[SEP]`: Separates different segments (e.g., question from context)\n","- `[PAD]`: Fills shorter sequences to match batch length\n","- `[UNK]`: Replaces unknown words not in vocabulary"],"metadata":{"id":"WtckjzmWej0_"}},{"cell_type":"markdown","source":["# Batch Processing\n","\n","Models are most efficient when they process multiple texts at once (a *batch*). However, a batch of texts will likely have different lengths. To solve this, we pad the shorter sequences with the `[PAD]` token until they are all as long as the longest sequence.\n","\n","But if we do this, how does the model know to ignore the padding? That's where the attention mask comes in."],"metadata":{"id":"fi2IBpgEXS1z"}},{"cell_type":"markdown","source":["# Attention Masks and Batch Processing\n","\n","**Attention masks** tell the model which tokens to pay attention to: 1 means real token (pay attention) and 0 means padding token (ignore).\n","\n","For **batch processing**, all sequences must be the same length. For example, \"Hello\" becomes [CLS] Hello [SEP] [PAD] [PAD], \"Hello world\" becomes [CLS] Hello world [SEP] [PAD], and \"Hello world today\" becomes [CLS] Hello world today [SEP].\n","\n","```python\n","batch_encoding = tokenizer(\n","    batch_texts,\n","    padding=True,\n","    truncation=True,\n","    return_tensors=\"pt\"\n",")\n","```\n","\n","`padding=True` adds [PAD] tokens to make all sequences the same length, `truncation=True` cuts sequences longer than max length, and `return_tensors=\"pt\"` returns PyTorch tensors (not lists).\n","\n","Let's say you have a sentence like this:\n","\n","> The cat sat on the mat.\n","\n","But your magic flashlight can only shine on **important words**—you don’t want to waste light on words that aren’t real or just there to fill space.\n","\n","Sometimes, you’re given a sentence like:\n","\n","> The cat sat\n","\n","But to make it the same size as other sentences, your grown-up teacher adds some blank spaces like this:\n","\n","> The cat sat `[PAD]` `[PAD]` `[PAD]`\n","\n","You only want to look at the real words, not the `[PAD]` ones. So\n","\n","An **attention map** is like a little map that says\"\n","\n","> \"Hey brain! Only pay attention to the **real** words. Ignore the fake ones!\"\n","\n","So you get a mask like this:\n","\n","```\n","[1, 1, 1, 0, 0, 0]\n","```\n","\n","Each number is like a light switch:\n","\n","- `1` = \"Look at this word!\"\n","- `0` = \"Skip this one. It's just padding.\"\n","\n","\n","## Why Do We Need It?\n","\n","Because the model is trying to learn what words mean and how they connect—but it would get confused if it starts thinking `[PAD]` means something. Attention masks help it **focus** only on the real stuff.\n","\n","So the model says: “Got it! I'll only pay attention to the first 4 words.”"],"metadata":{"id":"9BaGEM8aa4VN"}},{"cell_type":"code","source":["texts = [\n","    \"The cat sat on the mat.\",\n","    \"The cat sat.\",\n","]\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","inputs = tokenizer(texts, padding=True, return_tensors=\"pt\")\n","\n","print(inputs[\"input_ids\"])\n","print(inputs[\"attention_mask\"])"],"metadata":{"id":"epCwMIP_cs6H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tokenization is the foundation of all NLP tasks. All of the strings of text that we've processed in our previous time together went through this process automatically. Understanding tokenization helps you debug issues with model inputs, optimize for your specific use case, understand model limitations, and build more sophisticated applications."],"metadata":{"id":"b9xgHRmSeLUb"}}]}