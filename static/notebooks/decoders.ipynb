{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","cell_execution_strategy":"setup","authorship_tag":"ABX9TyPJ05KiJByna2UcUBGbHMr5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# @title Install the Dependencies and Set Everything Up {\"display-mode\": \"form\"}\n","\n","\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","import numpy as np\n","import seaborn as sns\n","from google.colab import output\n","\n","# Suppress verbose output\n","from transformers import logging\n","logging.set_verbosity_error()\n","\n","print(\"üé∏ Libraries installed and imported successfully.\")"],"metadata":{"id":"rhFZdF0iiukG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Decoders: Generating Text with Models like GPT\n","\n","Let's talk about how models like GPT-2 are masterfully designed to **generate new text**. We'll uncover how these models write stories, answer questions, and complete your sentences. By the end, you'll understand the architecture and step-by-step process of text generation."],"metadata":{"id":"Syf9zMV1iaqB"}},{"cell_type":"markdown","source":["# The Core Idea: Causal Attention (or, Masked Self-Attention)\n","\n","Imagine you're reading a mystery novel one page at a time‚Äîyou can see everything that has already happened, but you‚Äôre not allowed to peek at future pages.  A **causal attention mask** makes a language model behave the same way.  When the model is generating the next word, the mask hides (‚Äúmasks out‚Äù) all the words that come **after** the current position, so the model can only ‚Äúattend‚Äù to words it has already produced.  This one-way window keeps the model from cheating by looking ahead and ensures each new word depends solely on the past context, just like a reader who hasn‚Äôt turned the next page yet.\n","\n","The most important difference between a text-generation model (decoder) and a text-understanding model (encoder) is how they \"see\" the input.\n","\n","* **Encoder (like BERT):** Uses **bidirectional attention**. When analyzing the word \"sat\" in \"the cat sat on the mat,\" it can look at words both before (\"the cat\") and after (\"on the mat\"). This is great for understanding the full context.\n","* **Decoder (like GPT):** Uses **causal attention**. When *generating* the word \"sat,\" it can only see the words that came before it (\"the cat\"). It cannot see the future.\n","\n","Why is this restriction necessary? Because when you're generating text, the future words don't exist yet! The model _must_ predict the next word based only on what it has already written.\n","\n","**Why it matters:** Preventing a model from seeing future tokens during training teaches it the natural left-to-right flow of language.  At inference time‚Äîwhen it‚Äôs writing text for you‚Äîthe same masking lets it roll forward word by word, using only what it has already written to decide what comes next.\n"],"metadata":{"id":"9b0YIKCDjE_i"}},{"cell_type":"markdown","source":["# Visualizing the Causal Attention Mask\n","\n","Let's see what this looks like in practice. The causal mask is a matrix that explicitly prevents the model from attending to future tokens by setting their attention scores to `-infinity` before the softmax step."],"metadata":{"id":"IPCjQud9jVSV"}},{"cell_type":"code","source":["# Let's create a dummy sequence of 5 tokens\n","sequence_length = 5\n","# The attention mask will be a 5x5 matrix\n","causal_mask = torch.triu(torch.ones(sequence_length, sequence_length) * -1e9, diagonal=1)\n","\n","# Let's make a pretty little chart.\n","plt.figure(figsize=(6, 6))\n","sns.heatmap(causal_mask, cmap='viridis', annot=True, fmt=\".0f\", cbar=False,\n","            xticklabels=np.arange(sequence_length), yticklabels=np.arange(sequence_length))\n","plt.title(\"Causal Attention Mask Heatmap\")\n","plt.xlabel(\"Key Token Index\")\n","plt.ylabel(\"Query Token Index\")\n","plt.show()"],"metadata":{"id":"H7CQRKY2jgBs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the mask above, `0.` means \"pay attention,\" and a large negative number means \"ignore.\" This ensures that at each step, the model is only influenced by the past."],"metadata":{"id":"-qfLL5aAjcIT"}},{"cell_type":"markdown","source":["# The Decoder Architecture: Stacking the Blocks\n","\n","A decoder-only model like GPT-2 is essentially a stack of identical \"decoder blocks.\"\n","\n","Each block has two main components:\n","\n","1.  **Masked Multi-Head Self-Attention:** This is the causal attention mechanism we just discussed. It allows the model to weigh the importance of previous words.\n","2.  **Feed-Forward Neural Network:** This processes the output from the attention layer, adding more computational depth to learn complex patterns.\n","\n","This stack of blocks processes the token embeddings. The output of the final block is fed into one last layer: a **language modeling head**. This is a linear layer that projects the final token representation into a massive vector‚Äîone score for every single word in the model's vocabulary. A **softmax** function then converts these scores into probabilities."],"metadata":{"id":"wIvZencnmLiY"}},{"cell_type":"code","source":["# Load a small pre-trained model (GPT-2) and its tokenizer\n","model_name = \"gpt2-medium\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","# Make sure the tokenizer has a padding token for batching\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","output.clear() # Clean up all of those progress bars.\n","\n","# Let's see the language modeling head\n","print(\"Language Modeling Head (Output Layer):\")\n","print(model.lm_head)\n","\n","# The output size matches the vocabulary size\n","print(f\"Vocabulary size: {tokenizer.vocab_size}\")"],"metadata":{"id":"gSR9EYUcmV67"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The Generation Process: Autoregressive Decoding\n","\n","**Autoregressive** is a *fancy* term for a simple idea: the model's next prediction depends on its own previous predictions. It's a step-by-step loop.\n","\n","1.  Start with a prompt.\n","2.  The model predicts probabilities for the very next token.\n","3.  A **decoding strategy** is used to select one token from that probability distribution.\n","4.  The selected token is added to the end of the input sequence.\n","5.  Repeat from step 2 until a stop token is generated or the maximum length is reached.\n","\n","The most interesting part of this process is step 3: how do we choose the next token?"],"metadata":{"id":"2Q4ouu3umSQb"}},{"cell_type":"markdown","source":["# Decoding Strategy 1: Greedy Search (The Simplest)\n","\n","Greedy search always picks the single most likely next token. It's fast and predictable, but often leads to boring and repetitive text because it never takes a risk on a slightly less probable but more interesting word."],"metadata":{"id":"MYSz2HsYpE0P"}},{"cell_type":"code","source":["prompt = \"The future of artificial intelligence is\"\n","inputs = tokenizer(prompt, return_tensors=\"pt\")\n","\n","# Generate text using greedy search\n","# max_new_tokens is the number of tokens to generate *after* the prompt\n","greedy_output = model.generate(inputs.input_ids, max_new_tokens=20, num_beams=1, do_sample=False)\n","\n","print(\"Greedy Search Output:\")\n","print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"],"metadata":{"id":"8EXwmGytpLdF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Decoding Strategy 2: Sampling with Temperature\n","\n","Instead of just picking the top token, we can *sample* from the probability distribution. This introduces randomness. We can control this randomness with a parameter called **temperature**.\n","\n","  * **Low Temperature (e.g., 0.2):** Makes the distribution \"peakier.\" The model becomes more confident and conservative, similar to greedy search.\n","  * **High Temperature (e.g., 1.5):** Flattens the distribution. The model takes more risks, leading to more creative or even nonsensical text.\n","\n","Let's visualize how temperature reshapes probabilities."],"metadata":{"id":"CcMA61qQpHzn"}},{"cell_type":"code","source":["# Get the model's predicted probabilities for the next word\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","    # Get the logits for the last token in the sequence\n","    next_token_logits = outputs.logits[:, -1, :]\n","    # Apply softmax to get probabilities\n","    probs = F.softmax(next_token_logits, dim=-1).cpu().numpy().flatten()\n","\n","# Get top 20 probabilities for visualization\n","topk_probs, topk_indices = torch.topk(torch.from_numpy(probs), 20)\n","topk_tokens = [tokenizer.decode(i) for i in topk_indices]\n","\n","# Function to apply temperature\n","def apply_temperature(logits, temperature):\n","    return F.softmax(logits / temperature, dim=-1).cpu().numpy().flatten()\n","\n","# Visualize the effect of temperature\n","plt.figure(figsize=(15, 5))\n","temperatures = [0.1, 0.7, 1.5]\n","\n","for i, temp in enumerate(temperatures):\n","    plt.subplot(1, len(temperatures), i + 1)\n","    temp_probs = apply_temperature(next_token_logits, temp)\n","    plt.bar(topk_tokens, temp_probs[topk_indices.numpy()], color='skyblue')\n","    plt.title(f\"Temperature = {temp}\")\n","    plt.xticks(rotation=90)\n","    plt.ylabel(\"Probability\")\n","plt.tight_layout()\n","plt.show()\n","\n","# Let's generate with sampling and temperature\n","sampling_output = model.generate(\n","    inputs.input_ids,\n","    max_new_tokens=50,\n","    do_sample=True, # Enable sampling\n","    temperature=0.7, # A balanced value for creativity\n","    top_k=0 # We'll discuss top_k next\n",")\n","\n","print(\"\\nSampling with Temperature (0.7) Output:\")\n","print(tokenizer.decode(sampling_output[0], skip_special_tokens=True))"],"metadata":{"id":"tWhbYIVrpjJL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Decoding Strategy 3: Top-K and Top-p (Nucleus) Sampling\n","\n","Sampling can sometimes produce very weird words if it randomly picks a token with a very low probability. To prevent this, we can filter the distribution before sampling.\n","\n","  * **Top-K Sampling:** Only consider the `k` most likely tokens for sampling. For example, with `top_k=50`, the model will only sample from the 50 most probable next words.\n","  * **Top-p (Nucleus) Sampling:** A more dynamic approach. It considers the smallest set of tokens whose cumulative probability exceeds a threshold `p`. If `p=0.9`, it samples from the top tokens that make up 90% of the probability mass. This adapts to the situation: when the model is very certain, it might only consider a few words; when it's uncertain, it might consider many.\n","\n","Top-p is generally the recommended sampling method for high-quality text generation."],"metadata":{"id":"FLiftvKHpe5F"}},{"cell_type":"code","source":["# Generate with Top-p (Nucleus) Sampling\n","nucleus_output = model.generate(\n","    inputs.input_ids,\n","    max_new_tokens=50,\n","    do_sample=True,\n","    top_p=0.92, # Use nucleus sampling\n","    top_k=0 # Make sure top_k is disabled\n",")\n","\n","print(tokenizer.decode(nucleus_output[0], skip_special_tokens=True))"],"metadata":{"id":"CeVyJjclp4ez"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Common Limitations\n","\n","  * **Context Window:** Models have a finite memory. GPT-2, for example, has a context window of 1024 tokens. It cannot remember anything that happened before that window.\n","  * **Repetition:** Models can sometimes get stuck in repetitive loops. Parameters like `repetition_penalty` can help mitigate this.\n","  * **Hallucination:** A model can generate text that sounds plausible but is factually incorrect or nonsensical. It has no true understanding of the world; it's a pattern-matching machine.\n","  * **Bias:** These models are trained on vast amounts of internet text, and they inherit the societal biases (both good and bad) present in that data. Always be critical of the output."],"metadata":{"id":"3_qhNZXop0rT"}},{"cell_type":"markdown","source":["# Stopping Mid-Sentence\n","\n","Models learn to end sentences naturally by predicting a special **end-of-sequence (EOS) token**.\n","\n","During their training on vast amounts of text, models like GPT observe that sentences and paragraphs consistently end with a certain pattern. They learn to associate the completion of a thought or sentence with the prediction of this specific `[EOS]` token.\n","\n","## The Generation Process\n","\n","When a model generates text, it performs a step-by-step process at each stage:\n","\n","1.  It calculates the probability for every possible token in its vocabulary that could come next.\n","2.  The special `[EOS]` token is included in this vocabulary and is assigned a probability just like any other word.\n","3.  As the generated sentence starts to form a complete thought, the model's training makes it much more likely to predict the `[EOS]` token as the next logical item.\n","4.  Once the decoding strategy (like top-p sampling or greedy search) selects the `[EOS]` token, the generation process stops.\n","\n","This learned behavior is why the model can conclude a sentence naturally. Our previous examples often stopped midway because we used the `max_new_tokens` parameter, which acts as a hard cutoff regardless of whether the sentence was finished."],"metadata":{"id":"lPHFQR3aqnQL"}},{"cell_type":"code","source":["# Increase max_new_tokens to allow for a full sentence\n","natural_stop_output = model.generate(\n","    inputs.input_ids,\n","    max_new_tokens=100, # Give the model plenty of room\n","    do_sample=True,\n","    top_p=0.92,\n","    temperature=0.7,\n","    top_k=0,\n","    # The function automatically uses the model's configured EOS token ID\n","    eos_token_id=tokenizer.eos_token_id\n",")\n","\n","print(tokenizer.decode(natural_stop_output[0], skip_special_tokens=True))"],"metadata":{"id":"86jctumBrV3a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Text Generation versus Question Answering\n","\n","At their core, models like ChatGPT and Gemini are highly advanced **text-generation systems**. They perform tasks like question-answering *by* generating text that happens to be the correct answer.\n","\n","They don't switch between different \"modes\" like the `pipeline` tasks. Instead, their single, powerful text-generation ability has been trained to be so good at recognizing patterns that it can produce the right kind of text for almost any task you give it.\n","\n","* **When you ask a question**, it generates text that is an answer.\n","* **When you ask for a summary**, it generates text that is a summary.\n","* **When you ask it to translate**, it generates text in the target language.\n","\n","It's all about predicting the most appropriate next token to fulfill your request.\n","\n","## Extractive vs. Generative Question Answering\n","\n","* **Extractive QA** üå≠: In example where we pull out content from a brief blurb on the history of hot dog, what we used was *extractive*. It was limited to finding and pulling the exact answer directly from the context you provided. It couldn't use any outside knowledge.\n","* **Generative QA** ü§ñ: ChatGPT, Claude, Gemini, and friends perform *generative* (or abstractive) question-answering. They don't search a provided text; they generate the answer from scratch based on the vast knowledge learned during their training. This is why they can answer questions on almost any topic without needing a specific context document.\n","\n","## The Secret Sauce: Instruction Fine-Tuning\n","\n","The reason these models are so good at following instructions‚Äîlike answering questions‚Äîcomes from an extra training step after the initial text-generation pre-training.\n","\n","1.  **Base Model**: They start as a _massive_ decoder model (like `gpt2-medium` but _exponentially_ larger) that's been trained on a huge portion of the internet to just predict the next word.\n","2.  **Instruction Fine-Tuning ‚úÖ**: After this, the model is further trained on a high-quality, curated dataset of `[instruction, response]` pairs that were written by humans. This step specifically teaches the model how to be a helpful assistant‚Äîhow to answer questions, follow commands, and format its output in a useful way.\n","\n","So, while the fundamental mechanism is still text generation, this specialized fine-tuning is what turns a general-purpose text predictor into a powerful and helpful conversational AI."],"metadata":{"id":"dV6ZjEWatQr7"}},{"cell_type":"markdown","source":["Models figure out how to respond to a question through a specialized training process called **instruction fine-tuning**. The model learns that the correct text generation following a question is an answer, not a continuation of the question itself.\n","\n","Think of it like this:\n","\n","  * **A base model** is like a student who has read an entire library. If you give them the start of a sentence, \"The definition of photosynthesis is...,\" they're very good at completing it based on the patterns they've seen.\n","  * **An instruction-tuned model** is like that same student after they've taken a class where they practiced answering questions. They've been explicitly taught that when they see a phrase ending in a question mark, the correct response is to provide an answer.\n","\n","## The Fine-Tuning Process\n","\n","During instruction fine-tuning, the model is shown millions of high-quality examples that look like this:\n","\n","```json\n","{\n","  \"instruction\": \"What is the capital of France?\",\n","  \"response\": \"The capital of France is Paris.\"\n","}\n","```\n","\n","By training on this data, the model learns a powerful new pattern: when the input text is a question, the statistically likeliest \"next tokens\" are the ones that form a coherent answer. This process adjusts the model's internal weights so that generating an answer becomes the highest probability path.\n","\n","So, the model isn't \"deciding\" to answer your question. It's simply that its training has made the text of an answer the most probable continuation of the text of your prompt."],"metadata":{"id":"a1pi3jEBuOOw"}},{"cell_type":"markdown","source":["# Conclusion\n","\n","  * **Causal Attention:** The core mechanism that forces a model to only look at the past when generating text.\n","  * **Decoder Architecture:** How Masked Attention and Feed-Forward layers are stacked to create models like GPT.\n","  * **Autoregressive Generation:** The step-by-step process of predicting one token at a time.\n","  * **Decoding Strategies:** The different methods (Greedy, Temperature, Top-p) for choosing the next token, trading off between safety and creativity.\n","\n","You are now equipped with the fundamental knowledge of how modern text generation models work under the hood. The \"magic\" of `pipeline(\"text-generation\")` should now be way less magical."],"metadata":{"id":"teFC_MUEqHHR"}}]}